{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import pickle\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-ffe7336311f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/rui/Documents/repositories/m5_forecasting_accuracy/v01000/features/add_weight.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/repositories/m5_forecasting_accuracy/.venv/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;31m# We want to silence any warnings about, e.g. moved modules.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;31m# e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle(\"/Users/rui/Documents/repositories/m5_forecasting_accuracy/v01000/features/add_weight.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                   id  item_id  dept_id  cat_id  store_id  \\\n700176  HOBBIES_1_008_CA_1_validation     1444        3       1         0   \n700177  HOBBIES_1_009_CA_1_validation     1445        3       1         0   \n700178  HOBBIES_1_010_CA_1_validation     1446        3       1         0   \n700179  HOBBIES_1_012_CA_1_validation     1448        3       1         0   \n700180  HOBBIES_1_015_CA_1_validation     1451        3       1         0   \n\n        state_id     d  sales       date  wm_yr_wk  ...  sales_rolling_MAX_t7  \\\n700176         0  d_56      0 2011-03-25     11108  ...                  10.0   \n700177         0  d_56      0 2011-03-25     11108  ...                   5.0   \n700178         0  d_56      0 2011-03-25     11108  ...                   1.0   \n700179         0  d_56      1 2011-03-25     11108  ...                   3.0   \n700180         0  d_56      1 2011-03-25     11108  ...                  18.0   \n\n        sales_rolling_MAX_t30  sales_rolling_MAX_t60  sales_rolling_SKEW_t30  \\\n700176                    NaN                    NaN                     NaN   \n700177                    NaN                    NaN                     NaN   \n700178                    NaN                    NaN                     NaN   \n700179                    NaN                    NaN                     NaN   \n700180                    NaN                    NaN                     NaN   \n\n        sales_rolling_KURT_t30  sell_price_lag_t28  \\\n700176                     NaN            0.419922   \n700177                     NaN            1.559570   \n700178                     NaN            3.169922   \n700179                     NaN            5.980469   \n700180                     NaN            0.720215   \n\n        sell_price_price_change_t365  sell_price_rolling_price_std_t7  \\\n700176                           NaN                              0.0   \n700177                           NaN                              0.0   \n700178                           NaN                              0.0   \n700179                           NaN                              0.0   \n700180                           NaN                              0.0   \n\n        sell_price_rolling_price_std_t30    weight  \n700176                               NaN  0.678711  \n700177                               NaN  0.714355  \n700178                               NaN  0.142822  \n700179                               NaN  0.392822  \n700180                               NaN  0.500000  \n\n[5 rows x 80 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>item_id</th>\n      <th>dept_id</th>\n      <th>cat_id</th>\n      <th>store_id</th>\n      <th>state_id</th>\n      <th>d</th>\n      <th>sales</th>\n      <th>date</th>\n      <th>wm_yr_wk</th>\n      <th>...</th>\n      <th>sales_rolling_MAX_t7</th>\n      <th>sales_rolling_MAX_t30</th>\n      <th>sales_rolling_MAX_t60</th>\n      <th>sales_rolling_SKEW_t30</th>\n      <th>sales_rolling_KURT_t30</th>\n      <th>sell_price_lag_t28</th>\n      <th>sell_price_price_change_t365</th>\n      <th>sell_price_rolling_price_std_t7</th>\n      <th>sell_price_rolling_price_std_t30</th>\n      <th>weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>700176</th>\n      <td>HOBBIES_1_008_CA_1_validation</td>\n      <td>1444</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>d_56</td>\n      <td>0</td>\n      <td>2011-03-25</td>\n      <td>11108</td>\n      <td>...</td>\n      <td>10.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.419922</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.678711</td>\n    </tr>\n    <tr>\n      <th>700177</th>\n      <td>HOBBIES_1_009_CA_1_validation</td>\n      <td>1445</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>d_56</td>\n      <td>0</td>\n      <td>2011-03-25</td>\n      <td>11108</td>\n      <td>...</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.559570</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.714355</td>\n    </tr>\n    <tr>\n      <th>700178</th>\n      <td>HOBBIES_1_010_CA_1_validation</td>\n      <td>1446</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>d_56</td>\n      <td>0</td>\n      <td>2011-03-25</td>\n      <td>11108</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.169922</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.142822</td>\n    </tr>\n    <tr>\n      <th>700179</th>\n      <td>HOBBIES_1_012_CA_1_validation</td>\n      <td>1448</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>d_56</td>\n      <td>1</td>\n      <td>2011-03-25</td>\n      <td>11108</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.980469</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.392822</td>\n    </tr>\n    <tr>\n      <th>700180</th>\n      <td>HOBBIES_1_015_CA_1_validation</td>\n      <td>1451</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>d_56</td>\n      <td>1</td>\n      <td>2011-03-25</td>\n      <td>11108</td>\n      <td>...</td>\n      <td>18.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.720215</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.500000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 80 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_eval_submit(df, pred_interval=28):\n",
    "    latest_date = df['date'].max()\n",
    "    submit_date = latest_date - datetime.timedelta(days=pred_interval)\n",
    "    submit_mask = (df[\"date\"] > submit_date)\n",
    "\n",
    "    eval_date = latest_date - datetime.timedelta(days=pred_interval * 2)\n",
    "    eval_mask = ((df[\"date\"] > eval_date) & (df[\"date\"] <= submit_date))\n",
    "\n",
    "    train_mask = ((~eval_mask) & (~submit_mask))\n",
    "    return df[train_mask], df[eval_mask], df[submit_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train.columns.tolist()\n",
    "cols_to_drop = ['id', 'wm_yr_wk', 'd', 'date'] + ['sales']\n",
    "features = [f for f in features if f not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, eval_data, submit_data = split_train_eval_submit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = load_pickle(f'result/model/v01003.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [m.predict(eval_data[features].values,num_iteration=m.best_iteration) for m in models]\n",
    "preds = np.mean(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(preds, actual, weight=None):\n",
    "    return np.sqrt(mean_squared_log_error(actual, preds, sample_weight=weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "RMSE: 2.2041845139520113\nRMSLE: 0.5234763492320385\nWRMSSE: 111111\n"
    }
   ],
   "source": [
    "score = {}\n",
    "score['RMSE'] = mean_squared_error(eval_data['sales'].values, preds, squared=False)\n",
    "score['RMSLE'] = rmsle(preds, eval_data['sales'].values)\n",
    "# TODO: WRMSSEのscoreを算出できるようにする。\n",
    "score['WRMSSE'] = 111111  # e.score(valid_pred_df)\n",
    "for key, val in score.items():\n",
    "    print(f'{key}: {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_wrmsse(train_data, eval_data, preds):\n",
    "    def reverse_map(d):\n",
    "        return {v: k for k, v in d.items()}\n",
    "\n",
    "    # Processing train data.\n",
    "    idx_cols = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    train_idx_labels = pd.pivot_table(\n",
    "        train_data,\n",
    "        index=idx_cols,\n",
    "        columns='d',\n",
    "        values='sales'\n",
    "    ).reset_index()\n",
    "    del train_data; gc.collect()\n",
    "\n",
    "    d_cols = train_idx_labels.drop(idx_cols, axis=1).columns.tolist()\n",
    "    d_cols = sorted(d_cols, key=lambda x: int((re.search(r\"\\d+\", x)).group(0)))\n",
    "    # Decode map.\n",
    "    with open('features/encode_map.pkl', 'rb') as file:\n",
    "        encode_map = pickle.load(file)\n",
    "    for label_col, label_map in encode_map.items():\n",
    "        train_idx_labels[label_col] = train_idx_labels[label_col].map(reverse_map(label_map))\n",
    "    del encode_map; gc.collect()\n",
    "\n",
    "    train_idx_labels = train_idx_labels[idx_cols + d_cols]\n",
    "    # Processing eval label data.\n",
    "    eval_labels = pd.pivot_table(eval_data, index='id', columns='d', values='sales', fill_value=0).reset_index()\n",
    "    d_cols = eval_labels.drop('id', axis=1).columns.tolist()\n",
    "    d_cols = sorted(d_cols, key=lambda x: int((re.search(r\"\\d+\", x)).group(0)))\n",
    "    eval_labels = eval_labels[['id'] + d_cols]\n",
    "    # Processing eval predict data.\n",
    "    eval_data['pred'] = preds\n",
    "    pred_labels = pd.pivot_table(eval_data, index='id', columns='d', values='pred', fill_value=0).reset_index()\n",
    "    d_cols = pred_labels.drop('id', axis=1).columns.tolist()\n",
    "    d_cols = sorted(d_cols, key=lambda x: int((re.search(r\"\\d+\", x)).group(0)))\n",
    "    pred_labels = pred_labels[['id'] + d_cols]\n",
    "    # Estimate WRMSSE score.\n",
    "    calendar = pd.read_pickle('../data/reduced/calendar.pkl')\n",
    "    sell_prices = pd.read_pickle('../data/reduced/sell_prices.pkl')\n",
    "    e = WRMSSEEvaluator(train_idx_labels, eval_labels, calendar, sell_prices)\n",
    "    score = e.score(pred_labels)\n",
    "    return score\n",
    "\n",
    "print('WRMSSE:', estimate_wrmsse(train_data, eval_data, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRMSSEEvaluator(object):\n",
    "\n",
    "    group_ids = ('all_id', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id',\n",
    "                 ['state_id', 'cat_id'], ['state_id', 'dept_id'], ['store_id', 'cat_id'],\n",
    "                 ['store_id', 'dept_id'], ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_df: pd.DataFrame,\n",
    "                 valid_df: pd.DataFrame,\n",
    "                 calendar: pd.DataFrame,\n",
    "                 prices: pd.DataFrame):\n",
    "        '''\n",
    "        intialize and calculate weights\n",
    "        '''\n",
    "        self.calendar = calendar\n",
    "        self.prices = prices\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.train_target_columns = [i for i in self.train_df.columns if i.startswith('d_')]\n",
    "        self.weight_columns = self.train_df.iloc[:, -28:].columns.tolist()\n",
    "\n",
    "        self.train_df['all_id'] = \"all\"\n",
    "\n",
    "        self.id_columns = [i for i in self.train_df.columns if not i.startswith('d_')]\n",
    "        self.valid_target_columns = [i for i in self.valid_df.columns if i.startswith('d_')]\n",
    "\n",
    "        if not all([c in self.valid_df.columns for c in self.id_columns]):\n",
    "            self.valid_df = pd.concat([self.train_df[self.id_columns], self.valid_df],\n",
    "                                      axis=1,\n",
    "                                      sort=False)\n",
    "        self.train_series = self.trans_30490_to_42840(self.train_df,\n",
    "                                                      self.train_target_columns,\n",
    "                                                      self.group_ids)\n",
    "        self.valid_series = self.trans_30490_to_42840(self.valid_df,\n",
    "                                                      self.valid_target_columns,\n",
    "                                                      self.group_ids)\n",
    "        self.weights = self.get_weight_df()\n",
    "        self.scale = self.get_scale()\n",
    "        self.train_series = None\n",
    "        self.train_df = None\n",
    "        self.prices = None\n",
    "        self.calendar = None\n",
    "\n",
    "    def get_scale(self):\n",
    "        '''\n",
    "        scaling factor for each series ignoring starting zeros\n",
    "        '''\n",
    "        scales = []\n",
    "        for i in range(len(self.train_series)):\n",
    "            series = self.train_series.iloc[i].values\n",
    "            series = series[np.argmax(series != 0):]\n",
    "            scale = ((series[1:] - series[:-1]) ** 2).mean()\n",
    "            scales.append(scale)\n",
    "        return np.array(scales)\n",
    "\n",
    "    def get_name(self, i):\n",
    "        '''\n",
    "        convert a str or list of strings to unique string\n",
    "        used for naming each of 42840 series\n",
    "        '''\n",
    "        if type(i) == str or type(i) == int:\n",
    "            return str(i)\n",
    "        else:\n",
    "            return \"--\".join(i)\n",
    "\n",
    "    def get_weight_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        returns weights for each of 42840 series in a dataFrame\n",
    "        \"\"\"\n",
    "        day_to_week = self.calendar.set_index(\"d\")[\"wm_yr_wk\"].to_dict()\n",
    "        weight_df = self.train_df[[\"item_id\", \"store_id\"] + self.weight_columns].set_index(\n",
    "            [\"item_id\", \"store_id\"]\n",
    "        )\n",
    "        weight_df = (\n",
    "            weight_df.stack().reset_index().rename(columns={\"level_2\": \"d\", 0: \"value\"})\n",
    "        )\n",
    "        weight_df[\"wm_yr_wk\"] = weight_df[\"d\"].map(day_to_week)\n",
    "        weight_df = weight_df.merge(\n",
    "            self.prices, how=\"left\", on=[\"item_id\", \"store_id\", \"wm_yr_wk\"]\n",
    "        )\n",
    "        weight_df[\"value\"] = weight_df[\"value\"] * weight_df[\"sell_price\"]\n",
    "        weight_df = weight_df.set_index([\"item_id\", \"store_id\", \"d\"]).unstack(level=2)[\n",
    "            \"value\"\n",
    "        ]\n",
    "        weight_df = weight_df.loc[\n",
    "            zip(self.train_df.item_id, self.train_df.store_id), :\n",
    "        ].reset_index(drop=True)\n",
    "        weight_df = pd.concat(\n",
    "            [self.train_df[self.id_columns], weight_df], axis=1, sort=False\n",
    "        )\n",
    "        weights_map = {}\n",
    "        for i, group_id in enumerate(self.group_ids, leave=False):\n",
    "            lv_weight = weight_df.groupby(group_id)[self.weight_columns].sum().sum(axis=1)\n",
    "            lv_weight = lv_weight / lv_weight.sum()\n",
    "            for i in range(len(lv_weight)):\n",
    "                weights_map[self.get_name(lv_weight.index[i])] = np.array(\n",
    "                    [lv_weight.iloc[i]]\n",
    "                )\n",
    "        weights = pd.DataFrame(weights_map).T / len(self.group_ids)\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def trans_30490_to_42840(self, df, cols, group_ids, dis=False):\n",
    "        series_map = {}\n",
    "        for i, group_id in enumerate(self.group_ids):\n",
    "            tr = df.groupby(group_id)[cols].sum()\n",
    "            for i in range(len(tr)):\n",
    "                series_map[self.get_name(tr.index[i])] = tr.iloc[i].values\n",
    "        return pd.DataFrame(series_map).T\n",
    "\n",
    "    def get_rmsse(self, valid_preds) -> pd.Series:\n",
    "        score = ((self.valid_series - valid_preds) ** 2).mean(axis=1)\n",
    "        self.scale = np.where(self.scale != 0, self.scale, 1)\n",
    "        rmsse = (score / self.scale).map(np.sqrt)\n",
    "        return rmsse\n",
    "\n",
    "    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n",
    "        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n",
    "\n",
    "        if isinstance(valid_preds, np.ndarray):\n",
    "            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n",
    "\n",
    "        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds],\n",
    "                                axis=1,\n",
    "                                sort=False)\n",
    "        valid_preds = self.trans_30490_to_42840(valid_preds,\n",
    "                                                self.valid_target_columns,\n",
    "                                                self.group_ids,\n",
    "                                                True)\n",
    "        self.rmsse = self.get_rmsse(valid_preds)\n",
    "        self.contributors = pd.concat([self.weights, self.rmsse],\n",
    "                                      axis=1,\n",
    "                                      sort=False).prod(axis=1)\n",
    "        return np.sum(self.contributors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}