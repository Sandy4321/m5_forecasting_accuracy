{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v02000 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルにもデータに問題がありそうなので根本からBaselineを作り直す必要がある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Baseline を見直す\n",
    "    - 予測変数を対数変換しない。\n",
    "    - モデルを poission から regression に変更する\n",
    "- [ ] WRMSSE の評価関数クラスを定義し、LightGBM の評価関数として使う。\n",
    "- [ ] カテゴリ変数を指定する\n",
    "- [ ] カテゴリごとの標準化でスコアが改善するか試す\n",
    "- [ ] 特徴量の追加、Aggregated Sales Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "VERSION = 'v02000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_pickle(filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "def dump_pickle(data, filepath):\n",
    "    with open(filepath, 'wb') as file:\n",
    "        pickle.dump(data, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class WRMSSEEvaluator(object):\n",
    "    \n",
    "    group_ids = ( 'all_id', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id',\n",
    "        ['state_id', 'cat_id'],  ['state_id', 'dept_id'], ['store_id', 'cat_id'],\n",
    "        ['store_id', 'dept_id'], ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "\n",
    "    def __init__(self, \n",
    "                 train_df: pd.DataFrame, \n",
    "                 valid_df: pd.DataFrame, \n",
    "                 calendar: pd.DataFrame, \n",
    "                 prices: pd.DataFrame):\n",
    "        '''\n",
    "        intialize and calculate weights\n",
    "        '''\n",
    "        self.calendar = calendar\n",
    "        self.prices = prices\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.train_target_columns = [i for i in self.train_df.columns if i.startswith('d_')]\n",
    "        self.weight_columns = self.train_df.iloc[:, -28:].columns.tolist()\n",
    "\n",
    "        self.train_df['all_id'] = \"all\"\n",
    "\n",
    "        self.id_columns = [i for i in self.train_df.columns if not i.startswith('d_')]\n",
    "        self.valid_target_columns = [i for i in self.valid_df.columns if i.startswith('d_')]\n",
    "\n",
    "        if not all([c in self.valid_df.columns for c in self.id_columns]):\n",
    "            self.valid_df = pd.concat([self.train_df[self.id_columns], self.valid_df],\n",
    "                                      axis=1, \n",
    "                                      sort=False)\n",
    "        self.train_series = self.trans_30490_to_42840(self.train_df, \n",
    "                                                      self.train_target_columns, \n",
    "                                                      self.group_ids)\n",
    "        self.valid_series = self.trans_30490_to_42840(self.valid_df, \n",
    "                                                      self.valid_target_columns, \n",
    "                                                      self.group_ids)\n",
    "        self.weights = self.get_weight_df()\n",
    "        self.scale = self.get_scale()\n",
    "        self.train_series = None\n",
    "        self.train_df = None\n",
    "        self.prices = None\n",
    "        self.calendar = None\n",
    "\n",
    "    def get_scale(self):\n",
    "        '''\n",
    "        scaling factor for each series ignoring starting zeros\n",
    "        '''\n",
    "        scales = []\n",
    "        for i in tqdm(range(len(self.train_series))):\n",
    "            series = self.train_series.iloc[i].values\n",
    "            series = series[np.argmax(series!=0):]\n",
    "            scale = ((series[1:] - series[:-1]) ** 2).mean()\n",
    "            scales.append(scale)\n",
    "        return np.array(scales)\n",
    "    \n",
    "    def get_name(self, i):\n",
    "        '''\n",
    "        convert a str or list of strings to unique string \n",
    "        used for naming each of 42840 series\n",
    "        '''\n",
    "        if type(i) == str or type(i) == int:\n",
    "            return str(i)\n",
    "        else:\n",
    "            return \"--\".join(i)\n",
    "    \n",
    "    def get_weight_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        returns weights for each of 42840 series in a dataFrame\n",
    "        \"\"\"\n",
    "        day_to_week = self.calendar.set_index(\"d\")[\"wm_yr_wk\"].to_dict()\n",
    "        weight_df = self.train_df[[\"item_id\", \"store_id\"] + self.weight_columns].set_index(\n",
    "            [\"item_id\", \"store_id\"]\n",
    "        )\n",
    "        weight_df = (\n",
    "            weight_df.stack().reset_index().rename(columns={\"level_2\": \"d\", 0: \"value\"})\n",
    "        )\n",
    "        weight_df[\"wm_yr_wk\"] = weight_df[\"d\"].map(day_to_week)\n",
    "        weight_df = weight_df.merge(\n",
    "            self.prices, how=\"left\", on=[\"item_id\", \"store_id\", \"wm_yr_wk\"]\n",
    "        )\n",
    "        weight_df[\"value\"] = weight_df[\"value\"] * weight_df[\"sell_price\"]\n",
    "        weight_df = weight_df.set_index([\"item_id\", \"store_id\", \"d\"]).unstack(level=2)[\n",
    "            \"value\"\n",
    "        ]\n",
    "        weight_df = weight_df.loc[\n",
    "            zip(self.train_df.item_id, self.train_df.store_id), :\n",
    "        ].reset_index(drop=True)\n",
    "        weight_df = pd.concat(\n",
    "            [self.train_df[self.id_columns], weight_df], axis=1, sort=False\n",
    "        )\n",
    "        weights_map = {}\n",
    "        for i, group_id in enumerate(tqdm(self.group_ids, leave=False)):\n",
    "            lv_weight = weight_df.groupby(group_id)[self.weight_columns].sum().sum(axis=1)\n",
    "            lv_weight = lv_weight / lv_weight.sum()\n",
    "            for i in range(len(lv_weight)):\n",
    "                weights_map[self.get_name(lv_weight.index[i])] = np.array(\n",
    "                    [lv_weight.iloc[i]]\n",
    "                )\n",
    "        weights = pd.DataFrame(weights_map).T / len(self.group_ids)\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def trans_30490_to_42840(self, df, cols, group_ids, dis=False):\n",
    "        '''\n",
    "        transform 30490 sries to all 42840 series\n",
    "        '''\n",
    "        series_map = {}\n",
    "        for i, group_id in enumerate(tqdm(self.group_ids, leave=False, disable=dis)):\n",
    "            tr = df.groupby(group_id)[cols].sum()\n",
    "            for i in range(len(tr)):\n",
    "                series_map[self.get_name(tr.index[i])] = tr.iloc[i].values\n",
    "        return pd.DataFrame(series_map).T\n",
    "    \n",
    "    def get_rmsse(self, valid_preds) -> pd.Series:\n",
    "        '''\n",
    "        returns rmsse scores for all 42840 series\n",
    "        '''\n",
    "        score = ((self.valid_series - valid_preds) ** 2).mean(axis=1)\n",
    "        self.scale = np.where(self.scale != 0 , self.scale, 1)\n",
    "        rmsse = (score / self.scale).map(np.sqrt)\n",
    "        return rmsse\n",
    "\n",
    "    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n",
    "        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n",
    "\n",
    "        if isinstance(valid_preds, np.ndarray):\n",
    "            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n",
    "\n",
    "        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds],\n",
    "                                axis=1, \n",
    "                                sort=False)\n",
    "        valid_preds = self.trans_30490_to_42840(valid_preds, \n",
    "                                                self.valid_target_columns, \n",
    "                                                self.group_ids, \n",
    "                                                True)\n",
    "        self.rmsse = self.get_rmsse(valid_preds)\n",
    "        self.contributors = pd.concat([self.weights, self.rmsse], \n",
    "                                      axis=1, \n",
    "                                      sort=False).prod(axis=1)\n",
    "        return np.sum(self.contributors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calendar = pd.read_pickle('../data/reduced/calendar.pkl')\n",
    "# sell_prices = pd.read_pickle('../data/reduced/sell_prices.pkl')\n",
    "# train = pd.read_pickle('../data/reduced/sales_train_validation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('features/melted_and_merged_train.pkl')\n",
    "\n",
    "df = df[['id', 'd', 'sales', 'sell_price']]\n",
    "df['sales_value'] = df['sales'] * df['sell_price']\n",
    "df.drop(['sales', 'sell_price'], axis=1, inplace=True)\n",
    "\n",
    "weight_df = df.pivot(values='sales_value', index='id', columns='d')\n",
    "\n",
    "def ordered_d_cols(df_cols):\n",
    "    return sorted(df_cols, key=lambda x: int((re.search(r\"\\d+\", x)).group(0)))\n",
    "\n",
    "weight_df = weight_df[ordered_d_cols(weight_df.columns)]\n",
    "weight_df = weight_df.shift(28, axis=1).rolling(28, axis=1).sum()\n",
    "weight_df = weight_df / weight_df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df = weight_df.reset_index()\n",
    "weight_df = pd.melt(weight_df, id_vars='id', var_name='d', value_name='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>d</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_1_001_CA_1_validation</td>\n",
       "      <td>d_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOODS_1_001_CA_2_validation</td>\n",
       "      <td>d_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOODS_1_001_CA_3_validation</td>\n",
       "      <td>d_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOODS_1_001_CA_4_validation</td>\n",
       "      <td>d_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOODS_1_001_TX_1_validation</td>\n",
       "      <td>d_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id    d  weight\n",
       "0  FOODS_1_001_CA_1_validation  d_1     NaN\n",
       "1  FOODS_1_001_CA_2_validation  d_1     NaN\n",
       "2  FOODS_1_001_CA_3_validation  d_1     NaN\n",
       "3  FOODS_1_001_CA_4_validation  d_1     NaN\n",
       "4  FOODS_1_001_TX_1_validation  d_1     NaN"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
