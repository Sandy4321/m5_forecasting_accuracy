{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v02000 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルにもデータに問題がありそうなので根本からBaselineを作り直す必要がある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Baseline を見直す\n",
    "    - 予測変数を対数変換しない。\n",
    "    - モデルを poission から regression に変更する\n",
    "- [ ] WRMSSE の評価関数クラスを定義し、LightGBM の評価関数として使う。\n",
    "- [ ] カテゴリ変数を指定する\n",
    "- [ ] カテゴリごとの標準化でスコアが改善するか試す\n",
    "- [ ] 特徴量の追加、Aggregated Sales Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "VERSION = 'v02000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_pickle(filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "def dump_pickle(data, filepath):\n",
    "    with open(filepath, 'wb') as file:\n",
    "        pickle.dump(data, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class WRMSSEEvaluator(object):\n",
    "    \n",
    "    group_ids = ( 'all_id', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id',\n",
    "        ['state_id', 'cat_id'],  ['state_id', 'dept_id'], ['store_id', 'cat_id'],\n",
    "        ['store_id', 'dept_id'], ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "\n",
    "    def __init__(self, \n",
    "                 train_df: pd.DataFrame, \n",
    "                 valid_df: pd.DataFrame, \n",
    "                 calendar: pd.DataFrame, \n",
    "                 prices: pd.DataFrame):\n",
    "        '''\n",
    "        intialize and calculate weights\n",
    "        '''\n",
    "        self.calendar = calendar\n",
    "        self.prices = prices\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.train_target_columns = [i for i in self.train_df.columns if i.startswith('d_')]\n",
    "        self.weight_columns = self.train_df.iloc[:, -28:].columns.tolist()\n",
    "\n",
    "        self.train_df['all_id'] = \"all\"\n",
    "\n",
    "        self.id_columns = [i for i in self.train_df.columns if not i.startswith('d_')]\n",
    "        self.valid_target_columns = [i for i in self.valid_df.columns if i.startswith('d_')]\n",
    "\n",
    "        if not all([c in self.valid_df.columns for c in self.id_columns]):\n",
    "            self.valid_df = pd.concat([self.train_df[self.id_columns], self.valid_df],\n",
    "                                      axis=1, \n",
    "                                      sort=False)\n",
    "        self.train_series = self.trans_30490_to_42840(self.train_df, \n",
    "                                                      self.train_target_columns, \n",
    "                                                      self.group_ids)\n",
    "        self.valid_series = self.trans_30490_to_42840(self.valid_df, \n",
    "                                                      self.valid_target_columns, \n",
    "                                                      self.group_ids)\n",
    "        self.weights = self.get_weight_df()\n",
    "        self.scale = self.get_scale()\n",
    "        self.train_series = None\n",
    "        self.train_df = None\n",
    "        self.prices = None\n",
    "        self.calendar = None\n",
    "\n",
    "    def get_scale(self):\n",
    "        '''\n",
    "        scaling factor for each series ignoring starting zeros\n",
    "        '''\n",
    "        scales = []\n",
    "        for i in tqdm(range(len(self.train_series))):\n",
    "            series = self.train_series.iloc[i].values\n",
    "            series = series[np.argmax(series!=0):]\n",
    "            scale = ((series[1:] - series[:-1]) ** 2).mean()\n",
    "            scales.append(scale)\n",
    "        return np.array(scales)\n",
    "    \n",
    "    def get_name(self, i):\n",
    "        '''\n",
    "        convert a str or list of strings to unique string \n",
    "        used for naming each of 42840 series\n",
    "        '''\n",
    "        if type(i) == str or type(i) == int:\n",
    "            return str(i)\n",
    "        else:\n",
    "            return \"--\".join(i)\n",
    "    \n",
    "    def get_weight_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        returns weights for each of 42840 series in a dataFrame\n",
    "        \"\"\"\n",
    "        day_to_week = self.calendar.set_index(\"d\")[\"wm_yr_wk\"].to_dict()\n",
    "        weight_df = self.train_df[[\"item_id\", \"store_id\"] + self.weight_columns].set_index(\n",
    "            [\"item_id\", \"store_id\"]\n",
    "        )\n",
    "        weight_df = (\n",
    "            weight_df.stack().reset_index().rename(columns={\"level_2\": \"d\", 0: \"value\"})\n",
    "        )\n",
    "        weight_df[\"wm_yr_wk\"] = weight_df[\"d\"].map(day_to_week)\n",
    "        weight_df = weight_df.merge(\n",
    "            self.prices, how=\"left\", on=[\"item_id\", \"store_id\", \"wm_yr_wk\"]\n",
    "        )\n",
    "        weight_df[\"value\"] = weight_df[\"value\"] * weight_df[\"sell_price\"]\n",
    "        weight_df = weight_df.set_index([\"item_id\", \"store_id\", \"d\"]).unstack(level=2)[\n",
    "            \"value\"\n",
    "        ]\n",
    "        weight_df = weight_df.loc[\n",
    "            zip(self.train_df.item_id, self.train_df.store_id), :\n",
    "        ].reset_index(drop=True)\n",
    "        weight_df = pd.concat(\n",
    "            [self.train_df[self.id_columns], weight_df], axis=1, sort=False\n",
    "        )\n",
    "        weights_map = {}\n",
    "        for i, group_id in enumerate(tqdm(self.group_ids, leave=False)):\n",
    "            lv_weight = weight_df.groupby(group_id)[self.weight_columns].sum().sum(axis=1)\n",
    "            lv_weight = lv_weight / lv_weight.sum()\n",
    "            for i in range(len(lv_weight)):\n",
    "                weights_map[self.get_name(lv_weight.index[i])] = np.array(\n",
    "                    [lv_weight.iloc[i]]\n",
    "                )\n",
    "        weights = pd.DataFrame(weights_map).T / len(self.group_ids)\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def trans_30490_to_42840(self, df, cols, group_ids, dis=False):\n",
    "        '''\n",
    "        transform 30490 sries to all 42840 series\n",
    "        '''\n",
    "        series_map = {}\n",
    "        for i, group_id in enumerate(tqdm(self.group_ids, leave=False, disable=dis)):\n",
    "            tr = df.groupby(group_id)[cols].sum()\n",
    "            for i in range(len(tr)):\n",
    "                series_map[self.get_name(tr.index[i])] = tr.iloc[i].values\n",
    "        return pd.DataFrame(series_map).T\n",
    "    \n",
    "    def get_rmsse(self, valid_preds) -> pd.Series:\n",
    "        '''\n",
    "        returns rmsse scores for all 42840 series\n",
    "        '''\n",
    "        score = ((self.valid_series - valid_preds) ** 2).mean(axis=1)\n",
    "        self.scale = np.where(self.scale != 0 , self.scale, 1)\n",
    "        rmsse = (score / self.scale).map(np.sqrt)\n",
    "        return rmsse\n",
    "\n",
    "    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n",
    "        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n",
    "\n",
    "        if isinstance(valid_preds, np.ndarray):\n",
    "            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n",
    "\n",
    "        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds],\n",
    "                                axis=1, \n",
    "                                sort=False)\n",
    "        valid_preds = self.trans_30490_to_42840(valid_preds, \n",
    "                                                self.valid_target_columns, \n",
    "                                                self.group_ids, \n",
    "                                                True)\n",
    "        self.rmsse = self.get_rmsse(valid_preds)\n",
    "        self.contributors = pd.concat([self.weights, self.rmsse], \n",
    "                                      axis=1, \n",
    "                                      sort=False).prod(axis=1)\n",
    "        return np.sum(self.contributors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    files = ['calendar', 'sample_submission', 'sales_train_validation', 'sell_prices']\n",
    "\n",
    "    if os.path.exists('/kaggle/input/m5-forecasting-accuracy'):\n",
    "        data_dir_path = '/kaggle/input/m5-forecasting-accuracy'\n",
    "        dst_data = {}\n",
    "        for file in files:\n",
    "            print(f'Reading {file} ....')\n",
    "            dst_data[file] = pd.read_csv(data_dir_path + file + '.csv')\n",
    "    else:\n",
    "        data_dir_path = '../data/reduced/'\n",
    "        dst_data = {}\n",
    "        for file in files:\n",
    "            print(f'Reading {file} ....')\n",
    "            dst_data[file] = pd.read_pickle(data_dir_path + file + '.pkl')\n",
    "    return dst_data.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_map(filename='encode_map', use_cache=True):\n",
    "    filepath = f'features/{filename}.pkl'\n",
    "    if use_cache and os.path.exists(filepath):\n",
    "        print('Load cache of encode_map.')\n",
    "        return load_pickle(filepath)\n",
    "    \n",
    "    print('Processing encode_map.')\n",
    "    train = pd.read_pickle('../data/reduced/sales_train_validation.pkl')\n",
    "    categorical_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    encode_map = {\n",
    "        col: {label: i for i, label in enumerate(sorted(train[col].unique()))}\n",
    "        for col in categorical_cols\n",
    "    }\n",
    "\n",
    "    dump_pickle(encode_map, filepath)\n",
    "    return encode_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEMO: store_id, item_id, ごとの sell_price のランクを特徴量\n",
    "def parse_sell_price(filename='encoded_sell_price', use_cache=True):\n",
    "    filepath = f'features/{filename}.pkl'\n",
    "    if use_cache and os.path.exists(filepath):\n",
    "        print('Laod cache of parse_sell_price.')\n",
    "        return pd.read_pickle(filepath)\n",
    "    # Load Data\n",
    "    print('Processing parse_sell_price.')\n",
    "    df = pd.read_pickle('../data/reduced/sell_prices.pkl')\n",
    "    # Initial Processing And Feature Engineearing\n",
    "    df['Log1p_sell_price'] = np.log1p(df['sell_price'])\n",
    "    df['area_id'] = df['store_id'].str.extract('(\\w+)_\\d+')\n",
    "    df['sell_price_rate_by_wm_yr_wk__item_id'] = df['sell_price'] / \\\n",
    "        df.groupby(['wm_yr_wk', 'item_id'])['sell_price'].transform('mean')\n",
    "    df['sell_price_rate_by_wm_yr_wk__area__item_id'] = df['sell_price'] / \\\n",
    "        df.groupby(['wm_yr_wk', 'area_id', 'item_id'])['sell_price'].transform('mean')\n",
    "    df['sell_price_momentum'] = df['sell_price'] / \\\n",
    "        df.groupby(['store_id', 'item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "    \n",
    "    for g in ['store_id', 'item_id', ['store_id', 'item_id']]:\n",
    "        g_str = g if type(g) == str else '--'.join(g)\n",
    "        df[f'sell_price_rank_of_{g_str}'] = df.groupby(g)['sell_price'].transform(\n",
    "            lambda x: x.rank(method='min')).replace([np.inf, -np.inf], -1)\n",
    "        df[f'sell_price_pct_rank_of_{g_str}'] = df.groupby(g)['sell_price'].transform(\n",
    "            lambda x: x.rank(method='min', pct=True)).replace([np.inf, -np.inf], -1)\n",
    "    # Export DataFrame\n",
    "    df.drop(['area_id'], axis=1, inplace=True)\n",
    "    df = df.pipe(reduce_mem_usage)\n",
    "    df.to_pickle(filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_calendar(filename='encoded_calendar', use_cache=True):\n",
    "    filepath = f'features/{filename}.pkl'\n",
    "    if use_cache and os.path.exists(filepath):\n",
    "        print('Load cache of encode_calendar.')\n",
    "        return pd.read_pickle(filepath)\n",
    "    \n",
    "    # Load Data\n",
    "    print('Processing encode_calendar.')\n",
    "    df = pd.read_pickle('../data/reduced/calendar.pkl')\n",
    "    # Initial Processing And Feature Engineearing\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    attrs = [\n",
    "        \"quarter\",\n",
    "        \"month\",\n",
    "        \"weekofyear\",\n",
    "        \"day\",\n",
    "        \"dayofweek\",\n",
    "        \"is_year_end\",\n",
    "        \"is_year_start\",\n",
    "        \"is_quarter_end\",\n",
    "        \"is_quarter_start\",\n",
    "        \"is_month_end\",\n",
    "        \"is_month_start\",\n",
    "    ]\n",
    "    for attr in attrs:\n",
    "        df[attr] = getattr(df['date'].dt, attr).astype(np.int8)\n",
    "\n",
    "    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n",
    "    # MEMO: N_Unique of event_name_1 == 31 and event_name_2 == 5.\n",
    "    event_cols = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    df[event_cols] = df[event_cols].fillna('None')\n",
    "    for c in event_cols:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        df[c] = le.fit_transform(df[c].values)\n",
    "\n",
    "    # for c in event_cols:\n",
    "    #     for diff in [1, 2]:\n",
    "    #         df[f\"{c}_lag_t{diff}\"] = df[c].shift(diff)\n",
    "    # Drop columns.\n",
    "    cols_to_drop = ['weekday', 'wday']\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    # Export DataFrame\n",
    "    df = df.pipe(reduce_mem_usage)\n",
    "    df.to_pickle(filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hstack_sales_colums(df, latest_d, stack_days=28):\n",
    "    for i in range(latest_d, latest_d + stack_days):\n",
    "        df[f'd_{str(i + 1)}'] = 0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_data(filename='melted_train', use_cache=True):\n",
    "    filepath = f'features/{filename}.pkl'\n",
    "    # check is exist cached file.\n",
    "    if use_cache and os.path.exists(filepath):\n",
    "        print('Load Cache of melt_data.')\n",
    "        return pd.read_pickle(filepath)\n",
    "    # Load Data\n",
    "    print('Processing melt_data.')\n",
    "    latest_d = 1913  # latest_d of evaluation data is 1941\n",
    "    df = pd.read_pickle('../data/reduced/sales_train_validation.pkl')\n",
    "    df = hstack_sales_colums(df, latest_d)\n",
    "    # Melt Main Data and Join Optinal Data.\n",
    "    id_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    df = pd.melt(df, id_vars=id_columns, var_name='d', value_name='sales')\n",
    "    # Join calendar.\n",
    "    calendar = pd.read_pickle('features/encoded_calendar.pkl')\n",
    "    df = pd.merge(df, calendar, how='left', on='d')\n",
    "    # Join sell_price.\n",
    "    sell_price = pd.read_pickle('features/encoded_sell_price.pkl')\n",
    "    df = pd.merge(df, sell_price, how='left', on=['store_id', 'item_id', 'wm_yr_wk'])\n",
    "    # Label encoding main dataframe.\n",
    "    with open('features/encode_map.pkl', 'rb') as file:\n",
    "        encode_map = pickle.load(file)\n",
    "    for label, encode_map in encode_map.items():\n",
    "        df[label] = df[label].map(encode_map)\n",
    "    # Drop Null Records.\n",
    "    df.dropna(subset=['sell_price'], axis=0, inplace=True)\n",
    "    # Cache DataFrame.\n",
    "    df = df.pipe(reduce_mem_usage)\n",
    "    df.to_pickle(filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load cache of encode_map.\n",
      "Laod cache of parse_sell_price.\n",
      "Load cache of encode_calendar.\n",
      "Processing melt_data.\n",
      "Mem. usage decreased to 3800.34 Mb (32.0% reduction)\n",
      "\n",
      "Train DataFrame: (46881677, 41)\n",
      "Memory Usage: 3800.337357521057 Mb\n"
     ]
    }
   ],
   "source": [
    "_ = encode_map(filename='encode_map', use_cache=True)\n",
    "_ = parse_sell_price(filename='encoded_sell_price', use_cache=True)\n",
    "_ = encode_calendar(filename='encoded_calendar', use_cache=True)\n",
    "\n",
    "train = melt_data(filename='melted_train', use_cache=True)\n",
    "print('\\nTrain DataFrame:', train.shape)\n",
    "print('Memory Usage:', train.memory_usage().sum() / 1024 ** 2, 'Mb')\n",
    "# print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46881677, 41)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>quarter</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>is_year_end</th>\n",
       "      <th>is_year_start</th>\n",
       "      <th>is_quarter_end</th>\n",
       "      <th>is_quarter_start</th>\n",
       "      <th>is_month_end</th>\n",
       "      <th>is_month_start</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>Log1p_sell_price</th>\n",
       "      <th>sell_price_rate_by_wm_yr_wk__item_id</th>\n",
       "      <th>sell_price_rate_by_wm_yr_wk__area__item_id</th>\n",
       "      <th>sell_price_momentum</th>\n",
       "      <th>sell_price_rank_of_store_id</th>\n",
       "      <th>sell_price_pct_rank_of_store_id</th>\n",
       "      <th>sell_price_rank_of_item_id</th>\n",
       "      <th>sell_price_pct_rank_of_item_id</th>\n",
       "      <th>sell_price_rank_of_store_id--item_id</th>\n",
       "      <th>sell_price_pct_rank_of_store_id--item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
       "      <td>1444</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>12</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.459961</td>\n",
       "      <td>0.378418</td>\n",
       "      <td>0.983887</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4452.0</td>\n",
       "      <td>0.006374</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.025742</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.031921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOBBIES_1_009_CA_1_validation</td>\n",
       "      <td>1445</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.559570</td>\n",
       "      <td>0.939941</td>\n",
       "      <td>0.993652</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.125610</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.003679</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
       "      <td>1446</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.169922</td>\n",
       "      <td>1.427734</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.471436</td>\n",
       "      <td>2750.0</td>\n",
       "      <td>0.975098</td>\n",
       "      <td>267.0</td>\n",
       "      <td>0.946777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
       "      <td>1448</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.980469</td>\n",
       "      <td>1.943359</td>\n",
       "      <td>0.983398</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.785156</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HOBBIES_1_015_CA_1_validation</td>\n",
       "      <td>1451</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700195</td>\n",
       "      <td>0.530762</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14336.0</td>\n",
       "      <td>0.020538</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.055389</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.053192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  item_id  dept_id  cat_id  store_id  \\\n",
       "7   HOBBIES_1_008_CA_1_validation     1444        3       1         0   \n",
       "8   HOBBIES_1_009_CA_1_validation     1445        3       1         0   \n",
       "9   HOBBIES_1_010_CA_1_validation     1446        3       1         0   \n",
       "11  HOBBIES_1_012_CA_1_validation     1448        3       1         0   \n",
       "14  HOBBIES_1_015_CA_1_validation     1451        3       1         0   \n",
       "\n",
       "    state_id    d  sales       date  wm_yr_wk  month  year  event_name_1  \\\n",
       "7          0  d_1     12 2011-01-29     11101      1  2011            19   \n",
       "8          0  d_1      2 2011-01-29     11101      1  2011            19   \n",
       "9          0  d_1      0 2011-01-29     11101      1  2011            19   \n",
       "11         0  d_1      0 2011-01-29     11101      1  2011            19   \n",
       "14         0  d_1      4 2011-01-29     11101      1  2011            19   \n",
       "\n",
       "    event_type_1  event_name_2  event_type_2  snap_CA  snap_TX  snap_WI  \\\n",
       "7              2             3             1        0        0        0   \n",
       "8              2             3             1        0        0        0   \n",
       "9              2             3             1        0        0        0   \n",
       "11             2             3             1        0        0        0   \n",
       "14             2             3             1        0        0        0   \n",
       "\n",
       "    quarter  weekofyear  day  dayofweek  is_year_end  is_year_start  \\\n",
       "7         1           4   29          5            0              0   \n",
       "8         1           4   29          5            0              0   \n",
       "9         1           4   29          5            0              0   \n",
       "11        1           4   29          5            0              0   \n",
       "14        1           4   29          5            0              0   \n",
       "\n",
       "    is_quarter_end  is_quarter_start  is_month_end  is_month_start  \\\n",
       "7                0                 0             0               0   \n",
       "8                0                 0             0               0   \n",
       "9                0                 0             0               0   \n",
       "11               0                 0             0               0   \n",
       "14               0                 0             0               0   \n",
       "\n",
       "    is_weekend  sell_price  Log1p_sell_price  \\\n",
       "7            1    0.459961          0.378418   \n",
       "8            1    1.559570          0.939941   \n",
       "9            1    3.169922          1.427734   \n",
       "11           1    5.980469          1.943359   \n",
       "14           1    0.700195          0.530762   \n",
       "\n",
       "    sell_price_rate_by_wm_yr_wk__item_id  \\\n",
       "7                               0.983887   \n",
       "8                               0.993652   \n",
       "9                               1.000000   \n",
       "11                              0.983398   \n",
       "14                              1.000000   \n",
       "\n",
       "    sell_price_rate_by_wm_yr_wk__area__item_id  sell_price_momentum  \\\n",
       "7                                          1.0                  NaN   \n",
       "8                                          1.0                  NaN   \n",
       "9                                          1.0                  NaN   \n",
       "11                                         1.0                  NaN   \n",
       "14                                         1.0                  NaN   \n",
       "\n",
       "    sell_price_rank_of_store_id  sell_price_pct_rank_of_store_id  \\\n",
       "7                        4452.0                         0.006374   \n",
       "8                          -1.0                         0.125610   \n",
       "9                          -1.0                         0.471436   \n",
       "11                         -1.0                         0.785156   \n",
       "14                      14336.0                         0.020538   \n",
       "\n",
       "    sell_price_rank_of_item_id  sell_price_pct_rank_of_item_id  \\\n",
       "7                         71.0                        0.025742   \n",
       "8                         10.0                        0.003679   \n",
       "9                       2750.0                        0.975098   \n",
       "11                         1.0                        0.000355   \n",
       "14                       156.0                        0.055389   \n",
       "\n",
       "    sell_price_rank_of_store_id--item_id  \\\n",
       "7                                    9.0   \n",
       "8                                    1.0   \n",
       "9                                  267.0   \n",
       "11                                   1.0   \n",
       "14                                  15.0   \n",
       "\n",
       "    sell_price_pct_rank_of_store_id--item_id  \n",
       "7                                   0.031921  \n",
       "8                                   0.003546  \n",
       "9                                   0.946777  \n",
       "11                                  0.003546  \n",
       "14                                  0.053192  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ここまでは処理内容を担保済み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseFeature():\n",
    "    def __init__(self, filename, use_cache=True):\n",
    "        self.filepath = f'features/{filename}.pkl'\n",
    "        self.use_cache = use_cache\n",
    "        self.is_exist_cahce = False\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "    def __enter__(self):\n",
    "        if self.use_cache:\n",
    "            self.check_exist_cahce()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        if not self.is_exist_cahce:\n",
    "            with open(self.filepath, 'wb') as file:\n",
    "                pickle.dump(self.df, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def check_exist_cahce(self):\n",
    "        if os.path.exists(self.filepath):\n",
    "            self.is_exist_cahce = True\n",
    "\n",
    "    def get_feature(self, df):\n",
    "        if self.is_exist_cahce:\n",
    "            with open(self.filepath, 'rb') as file:\n",
    "                self.df = pickle.load(file)\n",
    "            return self.df\n",
    "        else:\n",
    "            self.df = self.create_feature(df)\n",
    "            return self.df\n",
    "\n",
    "    def create_feature(self, df):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddBaseSalesFeature(BaseFeature):\n",
    "    def create_feature(self, df):\n",
    "        print('Create Sales Feature.')\n",
    "        DAYS_PRED = 28\n",
    "        col = 'sales'\n",
    "        grouped_df = df.groupby([\"id\"])[col]\n",
    "\n",
    "        for diff in [0, 1, 2, 4, 5, 6]:\n",
    "            shift = DAYS_PRED + diff\n",
    "            df[f\"{col}_lag_t{shift}\"] = grouped_df.transform(lambda x: x.shift(shift))\n",
    "\n",
    "        for diff in [1, 2, 4, 5, 6, 7]:\n",
    "            df[f\"{col}_lag_t{shift}\"] = grouped_df.transform(\n",
    "                lambda x: x.shift(DAYS_PRED).diff(diff))\n",
    "            \n",
    "        for diff in [1, 2, 4, 5, 6, 7]:\n",
    "            df[f\"{col}_lag_t{shift}\"] = grouped_df.transform(\n",
    "                lambda x: x.shift(DAYS_PRED).pct_change(diff))\n",
    "\n",
    "        for window in [7, 30, 60, 90, 180]:\n",
    "            df[f\"{col}_rolling_STD_t{window}\"] = grouped_df.transform(\n",
    "                lambda x: x.shift(DAYS_PRED).rolling(window).std())\n",
    "\n",
    "        for window in [7, 30, 60, 90, 180]:\n",
    "            df[f\"{col}_rolling_MEAN_t{window}\"] = grouped_df.transform(\n",
    "                lambda x: x.shift(DAYS_PRED).rolling(window).mean())\n",
    "\n",
    "        for window in [7, 30, 60]:\n",
    "            df[f\"{col}_rolling_MIN_t{window}\"] = grouped_df.transform(\n",
    "                lambda x: x.shift(DAYS_PRED).rolling(window).min())\n",
    "\n",
    "        for window in [7, 30, 60]:\n",
    "            df[f\"{col}_rolling_MAX_t{window}\"] = grouped_df.transform(\n",
    "                lambda x: x.shift(DAYS_PRED).rolling(window).max())\n",
    "\n",
    "        for window in [7, 14, 30, 60]:\n",
    "            df[f\"{col}_rolling_ZeroRatio_t{window}\"] = grouped_df.transform(\n",
    "                lambda x: 1 - (x == 0).shift(DAYS_PRED).rolling(window).mean())\n",
    "            df[f\"{col}_rolling_ZeroCount_t{window}\"] = grouped_df.transform(\n",
    "                lambda x: (x == 0).shift(DAYS_PRED).rolling(window).sum())\n",
    "\n",
    "        df[f\"{col}_rolling_SKEW_t30\"] = grouped_df.transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(30).skew())\n",
    "        df[f\"{col}_rolling_KURT_t30\"] = grouped_df.transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(30).kurt())\n",
    "        return df.pipe(reduce_mem_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_null_rows(df):\n",
    "    print('Drop Null Rows.')\n",
    "    check_cols = ['lag', 'rolling']\n",
    "    cheked_regex = '|'.join(check_cols)\n",
    "    target_cols = df.columns[df.columns.str.contains(cheked_regex)]\n",
    "    is_contain_null_rows = df[target_cols].isnull().any(axis=1)\n",
    "    return df.loc[~(is_contain_null_rows), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_fe(data, target='sales'):\n",
    "    # rolling demand features\n",
    "    data['lag_t28'] = data.groupby(['id'])[target].transform(lambda x: x.shift(28))\n",
    "    data['lag_t29'] = data.groupby(['id'])[target].transform(lambda x: x.shift(29))\n",
    "    data['lag_t30'] = data.groupby(['id'])[target].transform(lambda x: x.shift(30))\n",
    "    data['rolling_mean_t7'] = data.groupby(['id'])[target].transform(lambda x: x.shift(28).rolling(7).mean())\n",
    "    data['rolling_std_t7'] = data.groupby(['id'])[target].transform(lambda x: x.shift(28).rolling(7).std())\n",
    "    data['rolling_mean_t30'] = data.groupby(['id'])[target].transform(lambda x: x.shift(28).rolling(30).mean())\n",
    "    data['rolling_mean_t90'] = data.groupby(['id'])[target].transform(lambda x: x.shift(28).rolling(90).mean())\n",
    "    data['rolling_mean_t180'] = data.groupby(['id'])[target].transform(lambda x: x.shift(28).rolling(180).mean())\n",
    "    data['rolling_std_t30'] = data.groupby(['id'])[target].transform(lambda x: x.shift(28).rolling(30).std())\n",
    "    data['rolling_skew_t30'] = data.groupby(['id'])[target].transform(lambda x: x.shift(28).rolling(30).skew())\n",
    "    data['rolling_kurt_t30'] = data.groupby(['id'])[target].transform(lambda x: x.shift(28).rolling(30).kurt()) \n",
    "    # price features\n",
    "    data['lag_price_t1'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "    data['price_change_t1'] = (data['lag_price_t1'] - data['sell_price']) / (data['lag_price_t1'])\n",
    "    data['rolling_price_max_t365'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n",
    "    data['price_change_t365'] = (data['rolling_price_max_t365'] - data['sell_price']) / (data['rolling_price_max_t365'])\n",
    "    data['rolling_price_std_t7'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n",
    "    data['rolling_price_std_t30'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std())\n",
    "    data.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)\n",
    "    # time features\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data['year'] = data['date'].dt.year\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['week'] = data['date'].dt.week\n",
    "    data['day'] = data['date'].dt.day\n",
    "    data['dayofweek'] = data['date'].dt.dayofweek\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, is_use_cache=True):\n",
    "    '''\n",
    "    TODO:\n",
    "        - 当該月の特徴量\n",
    "            - 月初・月末（１日）の売上\n",
    "            - 15, 20日などのクレジットカードの締日のごとの統計量\n",
    "        - 過去１ヶ月間の（特定item_idの売上 / スーパー全体の売上）\n",
    "            - （特定item_idの売上個数 / スーパー全体の売上個数）\n",
    "            - （特定item_idの売上個数 / スーパー全体の売上個数）\n",
    "    '''\n",
    "\n",
    "    with AddBaseSalesFeature(filename='add_sales_train', use_cache=is_use_cache) as feat:\n",
    "        df = feat.get_feature(df)\n",
    "\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame: (46881677, 73)\n",
      "Memory Usage: 6572.348258018494 Mb\n"
     ]
    }
   ],
   "source": [
    "# train = create_features(train, is_use_cache=True)\n",
    "train = simple_fe(train)\n",
    "print('Train DataFrame:', train.shape)\n",
    "print('Memory Usage:', train.memory_usage().sum() / 1024 ** 2, 'Mb')\n",
    "# print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46881677, 73)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>quarter</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>is_year_end</th>\n",
       "      <th>is_year_start</th>\n",
       "      <th>is_quarter_end</th>\n",
       "      <th>is_quarter_start</th>\n",
       "      <th>is_month_end</th>\n",
       "      <th>is_month_start</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>Log1p_sell_price</th>\n",
       "      <th>sell_price_rate_by_wm_yr_wk__item_id</th>\n",
       "      <th>sell_price_rate_by_wm_yr_wk__area__item_id</th>\n",
       "      <th>sell_price_momentum</th>\n",
       "      <th>sell_price_rank_of_store_id</th>\n",
       "      <th>sell_price_pct_rank_of_store_id</th>\n",
       "      <th>sell_price_rank_of_item_id</th>\n",
       "      <th>sell_price_pct_rank_of_item_id</th>\n",
       "      <th>sell_price_rank_of_store_id--item_id</th>\n",
       "      <th>sell_price_pct_rank_of_store_id--item_id</th>\n",
       "      <th>sales_lag_t28</th>\n",
       "      <th>sales_lag_t29</th>\n",
       "      <th>sales_lag_t30</th>\n",
       "      <th>sales_lag_t32</th>\n",
       "      <th>sales_lag_t33</th>\n",
       "      <th>sales_lag_t34</th>\n",
       "      <th>sales_rolling_STD_t7</th>\n",
       "      <th>sales_rolling_STD_t30</th>\n",
       "      <th>sales_rolling_STD_t60</th>\n",
       "      <th>sales_rolling_STD_t90</th>\n",
       "      <th>sales_rolling_STD_t180</th>\n",
       "      <th>sales_rolling_MEAN_t7</th>\n",
       "      <th>sales_rolling_MEAN_t30</th>\n",
       "      <th>sales_rolling_MEAN_t60</th>\n",
       "      <th>sales_rolling_MEAN_t90</th>\n",
       "      <th>sales_rolling_MEAN_t180</th>\n",
       "      <th>sales_rolling_MIN_t7</th>\n",
       "      <th>sales_rolling_MIN_t30</th>\n",
       "      <th>sales_rolling_MIN_t60</th>\n",
       "      <th>sales_rolling_MAX_t7</th>\n",
       "      <th>sales_rolling_MAX_t30</th>\n",
       "      <th>sales_rolling_MAX_t60</th>\n",
       "      <th>sales_rolling_ZeroRatio_t7</th>\n",
       "      <th>sales_rolling_ZeroCount_t7</th>\n",
       "      <th>sales_rolling_ZeroRatio_t14</th>\n",
       "      <th>sales_rolling_ZeroCount_t14</th>\n",
       "      <th>sales_rolling_ZeroRatio_t30</th>\n",
       "      <th>sales_rolling_ZeroCount_t30</th>\n",
       "      <th>sales_rolling_ZeroRatio_t60</th>\n",
       "      <th>sales_rolling_ZeroCount_t60</th>\n",
       "      <th>sales_rolling_SKEW_t30</th>\n",
       "      <th>sales_rolling_KURT_t30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
       "      <td>1444</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>12</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.459961</td>\n",
       "      <td>0.378418</td>\n",
       "      <td>0.983887</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4452.0</td>\n",
       "      <td>0.006374</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.025742</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.031921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_009_CA_1_validation</td>\n",
       "      <td>1445</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.559570</td>\n",
       "      <td>0.939941</td>\n",
       "      <td>0.993652</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.125610</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.003679</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
       "      <td>1446</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.169922</td>\n",
       "      <td>1.427734</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.471436</td>\n",
       "      <td>2750.0</td>\n",
       "      <td>0.975098</td>\n",
       "      <td>267.0</td>\n",
       "      <td>0.946777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
       "      <td>1448</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.980469</td>\n",
       "      <td>1.943359</td>\n",
       "      <td>0.983398</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.785156</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_015_CA_1_validation</td>\n",
       "      <td>1451</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700195</td>\n",
       "      <td>0.530762</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14336.0</td>\n",
       "      <td>0.020538</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.055389</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.053192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  item_id  dept_id  cat_id  store_id  \\\n",
       "0  HOBBIES_1_008_CA_1_validation     1444        3       1         0   \n",
       "1  HOBBIES_1_009_CA_1_validation     1445        3       1         0   \n",
       "2  HOBBIES_1_010_CA_1_validation     1446        3       1         0   \n",
       "3  HOBBIES_1_012_CA_1_validation     1448        3       1         0   \n",
       "4  HOBBIES_1_015_CA_1_validation     1451        3       1         0   \n",
       "\n",
       "   state_id    d  sales       date  wm_yr_wk  month  year  event_name_1  \\\n",
       "0         0  d_1     12 2011-01-29     11101      1  2011            19   \n",
       "1         0  d_1      2 2011-01-29     11101      1  2011            19   \n",
       "2         0  d_1      0 2011-01-29     11101      1  2011            19   \n",
       "3         0  d_1      0 2011-01-29     11101      1  2011            19   \n",
       "4         0  d_1      4 2011-01-29     11101      1  2011            19   \n",
       "\n",
       "   event_type_1  event_name_2  event_type_2  snap_CA  snap_TX  snap_WI  \\\n",
       "0             2             3             1        0        0        0   \n",
       "1             2             3             1        0        0        0   \n",
       "2             2             3             1        0        0        0   \n",
       "3             2             3             1        0        0        0   \n",
       "4             2             3             1        0        0        0   \n",
       "\n",
       "   quarter  weekofyear  day  dayofweek  is_year_end  is_year_start  \\\n",
       "0        1           4   29          5            0              0   \n",
       "1        1           4   29          5            0              0   \n",
       "2        1           4   29          5            0              0   \n",
       "3        1           4   29          5            0              0   \n",
       "4        1           4   29          5            0              0   \n",
       "\n",
       "   is_quarter_end  is_quarter_start  is_month_end  is_month_start  is_weekend  \\\n",
       "0               0                 0             0               0           1   \n",
       "1               0                 0             0               0           1   \n",
       "2               0                 0             0               0           1   \n",
       "3               0                 0             0               0           1   \n",
       "4               0                 0             0               0           1   \n",
       "\n",
       "   sell_price  Log1p_sell_price  sell_price_rate_by_wm_yr_wk__item_id  \\\n",
       "0    0.459961          0.378418                              0.983887   \n",
       "1    1.559570          0.939941                              0.993652   \n",
       "2    3.169922          1.427734                              1.000000   \n",
       "3    5.980469          1.943359                              0.983398   \n",
       "4    0.700195          0.530762                              1.000000   \n",
       "\n",
       "   sell_price_rate_by_wm_yr_wk__area__item_id  sell_price_momentum  \\\n",
       "0                                         1.0                  NaN   \n",
       "1                                         1.0                  NaN   \n",
       "2                                         1.0                  NaN   \n",
       "3                                         1.0                  NaN   \n",
       "4                                         1.0                  NaN   \n",
       "\n",
       "   sell_price_rank_of_store_id  sell_price_pct_rank_of_store_id  \\\n",
       "0                       4452.0                         0.006374   \n",
       "1                         -1.0                         0.125610   \n",
       "2                         -1.0                         0.471436   \n",
       "3                         -1.0                         0.785156   \n",
       "4                      14336.0                         0.020538   \n",
       "\n",
       "   sell_price_rank_of_item_id  sell_price_pct_rank_of_item_id  \\\n",
       "0                        71.0                        0.025742   \n",
       "1                        10.0                        0.003679   \n",
       "2                      2750.0                        0.975098   \n",
       "3                         1.0                        0.000355   \n",
       "4                       156.0                        0.055389   \n",
       "\n",
       "   sell_price_rank_of_store_id--item_id  \\\n",
       "0                                   9.0   \n",
       "1                                   1.0   \n",
       "2                                 267.0   \n",
       "3                                   1.0   \n",
       "4                                  15.0   \n",
       "\n",
       "   sell_price_pct_rank_of_store_id--item_id  sales_lag_t28  sales_lag_t29  \\\n",
       "0                                  0.031921            NaN            NaN   \n",
       "1                                  0.003546            NaN            NaN   \n",
       "2                                  0.946777            NaN            NaN   \n",
       "3                                  0.003546            NaN            NaN   \n",
       "4                                  0.053192            NaN            NaN   \n",
       "\n",
       "   sales_lag_t30  sales_lag_t32  sales_lag_t33  sales_lag_t34  \\\n",
       "0            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   sales_rolling_STD_t7  sales_rolling_STD_t30  sales_rolling_STD_t60  \\\n",
       "0                   NaN                    NaN                    NaN   \n",
       "1                   NaN                    NaN                    NaN   \n",
       "2                   NaN                    NaN                    NaN   \n",
       "3                   NaN                    NaN                    NaN   \n",
       "4                   NaN                    NaN                    NaN   \n",
       "\n",
       "   sales_rolling_STD_t90  sales_rolling_STD_t180  sales_rolling_MEAN_t7  \\\n",
       "0                    NaN                     NaN                    NaN   \n",
       "1                    NaN                     NaN                    NaN   \n",
       "2                    NaN                     NaN                    NaN   \n",
       "3                    NaN                     NaN                    NaN   \n",
       "4                    NaN                     NaN                    NaN   \n",
       "\n",
       "   sales_rolling_MEAN_t30  sales_rolling_MEAN_t60  sales_rolling_MEAN_t90  \\\n",
       "0                     NaN                     NaN                     NaN   \n",
       "1                     NaN                     NaN                     NaN   \n",
       "2                     NaN                     NaN                     NaN   \n",
       "3                     NaN                     NaN                     NaN   \n",
       "4                     NaN                     NaN                     NaN   \n",
       "\n",
       "   sales_rolling_MEAN_t180  sales_rolling_MIN_t7  sales_rolling_MIN_t30  \\\n",
       "0                      NaN                   NaN                    NaN   \n",
       "1                      NaN                   NaN                    NaN   \n",
       "2                      NaN                   NaN                    NaN   \n",
       "3                      NaN                   NaN                    NaN   \n",
       "4                      NaN                   NaN                    NaN   \n",
       "\n",
       "   sales_rolling_MIN_t60  sales_rolling_MAX_t7  sales_rolling_MAX_t30  \\\n",
       "0                    NaN                   NaN                    NaN   \n",
       "1                    NaN                   NaN                    NaN   \n",
       "2                    NaN                   NaN                    NaN   \n",
       "3                    NaN                   NaN                    NaN   \n",
       "4                    NaN                   NaN                    NaN   \n",
       "\n",
       "   sales_rolling_MAX_t60  sales_rolling_ZeroRatio_t7  \\\n",
       "0                    NaN                         NaN   \n",
       "1                    NaN                         NaN   \n",
       "2                    NaN                         NaN   \n",
       "3                    NaN                         NaN   \n",
       "4                    NaN                         NaN   \n",
       "\n",
       "   sales_rolling_ZeroCount_t7  sales_rolling_ZeroRatio_t14  \\\n",
       "0                         NaN                          NaN   \n",
       "1                         NaN                          NaN   \n",
       "2                         NaN                          NaN   \n",
       "3                         NaN                          NaN   \n",
       "4                         NaN                          NaN   \n",
       "\n",
       "   sales_rolling_ZeroCount_t14  sales_rolling_ZeroRatio_t30  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          NaN                          NaN   \n",
       "4                          NaN                          NaN   \n",
       "\n",
       "   sales_rolling_ZeroCount_t30  sales_rolling_ZeroRatio_t60  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          NaN                          NaN   \n",
       "4                          NaN                          NaN   \n",
       "\n",
       "   sales_rolling_ZeroCount_t60  sales_rolling_SKEW_t30  sales_rolling_KURT_t30  \n",
       "0                          NaN                     NaN                     NaN  \n",
       "1                          NaN                     NaN                     NaN  \n",
       "2                          NaN                     NaN                     NaN  \n",
       "3                          NaN                     NaN                     NaN  \n",
       "4                          NaN                     NaN                     NaN  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_eval_submit(df, pred_interval=28):\n",
    "    latest_date = df['date'].max()\n",
    "    submit_date = latest_date - datetime.timedelta(days=pred_interval)\n",
    "    submit_mask = (df[\"date\"] > submit_date)\n",
    "\n",
    "    eval_date = latest_date - datetime.timedelta(days=pred_interval * 2)\n",
    "    eval_mask = ((df[\"date\"] > eval_date) & (df[\"date\"] <= submit_date))\n",
    "\n",
    "    train_mask = ((~eval_mask) & (~submit_mask))\n",
    "    return df[train_mask], df[eval_mask], df[submit_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for LightGBM\n",
    "# class WRMSSEForLightGBM(WRMSSEEvaluator):\n",
    "#     def feval(self, preds, dtrain):\n",
    "#         preds = preds.reshape(self.valid_df[self.valid_target_columns].shape)\n",
    "#         score = self.score(preds)\n",
    "#         return 'WRMSSE', score, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RondomSeed_LGBM_Model():\n",
    "    def __init__(self, data, feature, target, n_fold, test_days, max_train_days, model_param, train_param, weight=None):\n",
    "        self.n_fold = n_fold\n",
    "        self.weight = weight\n",
    "        train_dataset, valid_dataset = self.split_data(\n",
    "            data, feature, target, test_days, max_train_days)\n",
    "        self.models = self.fit(train_dataset, valid_dataset, model_param, train_param)\n",
    "\n",
    "    def split_data(self, data, features, target, test_days, max_train_days):\n",
    "        latest_date = data['date'].max()\n",
    "        oldest_valid_date = latest_date - datetime.timedelta(days=test_days)\n",
    "        valid_mask = (data[\"date\"] > oldest_valid_date)\n",
    "        oldest_train_date = oldest_valid_date - datetime.timedelta(days=max_train_days)\n",
    "        train_mask = (data[\"date\"] > oldest_train_date) & (data[\"date\"] <= oldest_valid_date)\n",
    "\n",
    "        train_X, train_y = data.loc[train_mask, features], data.loc[train_mask, target]\n",
    "        valid_X, valid_y = data.loc[valid_mask, features], data.loc[valid_mask, target]\n",
    "\n",
    "        train_dataset = lgb.Dataset(train_X, label=train_y)\n",
    "        valid_dataset = lgb.Dataset(valid_X, label=valid_y, reference=train_dataset)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            train_dataset.set_weight(self.weight.loc[train_mask])\n",
    "            valid_dataset.set_weight(self.weight.loc[valid_mask])\n",
    "\n",
    "        print('Train DataFrame Size:', train_mask.sum())\n",
    "        print('Valid DataFrame Size:', valid_mask.sum())\n",
    "        return train_dataset, valid_dataset\n",
    "\n",
    "    def fit(self, train_dataset, valid_dataset, model_param, train_param,):\n",
    "        models = []\n",
    "        for n in range(self.n_fold):\n",
    "            print(f\"\\n{n + 1} of {self.n_fold} Fold:\\n\")\n",
    "            model_param['seed'] = model_param['seed'] + 1\n",
    "            model = lgb.train(\n",
    "                model_param,\n",
    "                train_dataset,\n",
    "                valid_sets=[train_dataset, valid_dataset],\n",
    "                valid_names=[\"train\", \"valid\"],\n",
    "                **train_param\n",
    "            )\n",
    "            models.append(model)\n",
    "        return models\n",
    "\n",
    "    def get_models(self):\n",
    "        return self.models\n",
    "\n",
    "    def predict(self, data):\n",
    "        models = self.get_models()\n",
    "        preds = [m.predict(data, num_iteration=m.best_iteration) for m in models]\n",
    "        avg_pred = np.mean(preds, axis=0)\n",
    "        return avg_pred\n",
    "\n",
    "    def save_importance(self, filepath, max_num_features=50, figsize=(20, 25), plot=False):\n",
    "        models = self.get_models()\n",
    "        # Define Feature Importance DataFrame.\n",
    "        imp_df = pd.DataFrame(\n",
    "            [m.feature_importance() for m in models],\n",
    "            columns=models[0].feature_name()\n",
    "        ).T\n",
    "        imp_df['AVG_Importance'] = imp_df.iloc[:, :len(models)].mean(axis=1)\n",
    "        imp_df['STD_Importance'] = imp_df.iloc[:, :len(models)].std(axis=1)\n",
    "        imp_df.sort_values(by='AVG_Importance', inplace=True)\n",
    "        # Plot Importance DataFrame.\n",
    "        plt.figure(figsize=figsize)\n",
    "        imp_df[-max_num_features:].plot(\n",
    "            kind='barh', title='Feature importance', figsize=figsize,\n",
    "            y='AVG_Importance', xerr='STD_Importance', align=\"center\"\n",
    "        )\n",
    "        if plot:\n",
    "            plt.show()\n",
    "        plt.savefig(filepath)\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, feature, target):\n",
    "    n_fold = 1\n",
    "    max_train_days = 4 * 365\n",
    "    test_days = 28\n",
    "\n",
    "#     model_param = {\n",
    "#         \"boosting_type\": \"gbdt\",\n",
    "#         \"metric\": \"rmse\",\n",
    "#         \"objective\": \"regression\",\n",
    "#         \"seed\": SEED,\n",
    "#         \"learning_rate\": 0.3,\n",
    "#         \"num_leaves\": 2**6,\n",
    "#         'min_data_in_leaf': 50,\n",
    "#         \"bagging_fraction\": 0.8,\n",
    "#         \"bagging_freq\": 1,\n",
    "#         \"feature_fraction\": 1.0,\n",
    "#         \"lambda_l2\": 0.1,\n",
    "#         \"verbosity\": -1\n",
    "#     }\n",
    "    \n",
    "    model_param = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'rmse',\n",
    "        'objective': 'regression',\n",
    "        'n_jobs': -1,\n",
    "        'seed': 236,\n",
    "        'learning_rate': 0.3,\n",
    "        'bagging_fraction': 0.75,\n",
    "        'bagging_freq': 10, \n",
    "        'colsample_bytree': 0.75\n",
    "    }\n",
    "\n",
    "    train_param = {\n",
    "        \"num_boost_round\": 100000,\n",
    "        \"early_stopping_rounds\": 50,\n",
    "        \"verbose_eval\": 100\n",
    "    }\n",
    "\n",
    "#     print(f'n_fold: {n_fold}')\n",
    "#     print(f'max_train_days: {max_train_days}')\n",
    "#     print(f'test_days: {test_days}')\n",
    "#     print(model_param)\n",
    "#     print(train_param)\n",
    "\n",
    "    df = drop_null_rows(df)\n",
    "    lgbm_model = RondomSeed_LGBM_Model(\n",
    "        df, feature, target, n_fold, test_days, max_train_days,\n",
    "        model_param, train_param\n",
    "    )\n",
    "    lgbm_model.save_importance(filepath=f'result/importance/{VERSION}.png')\n",
    "    dump_pickle(lgbm_model, f'result/model/{VERSION}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'sales'\n",
    "cols_to_drop = ['id', 'wm_yr_wk', 'd', 'date'] + [target]\n",
    "features = train.columns.tolist()\n",
    "features = [f for f in features if f not in cols_to_drop]\n",
    "# 関数の定義を変える。split_train_valid_eval_submit\n",
    "train_data, eval_data, submit_data = split_train_eval_submit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid_eval(df, pred_interval=28):\n",
    "    latest_date = df['date'].max()\n",
    "    eval_date = latest_date - datetime.timedelta(days=pred_interval)\n",
    "    eval_mask = (df[\"date\"] > eval_date)\n",
    "\n",
    "    valid_date = latest_date - datetime.timedelta(days=pred_interval * 2)\n",
    "    valid_mask = ((df[\"date\"] > valid_date) & (df[\"date\"] <= eval_date))\n",
    "\n",
    "    train_mask = ((~valid_mask) & (~eval_mask))\n",
    "    return df[train_mask], df[valid_mask], df[eval_mask]\n",
    "\n",
    "def run_lgb(data, features, target='sales'):\n",
    "    # going to evaluate with the last 28 days\n",
    "    train_data, valid_data, eval_data = split_train_valid_eval(data.iloc[-25000000:])\n",
    "    del data;gc.collect()\n",
    "\n",
    "    # define random hyperparammeters\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'rmse',\n",
    "        'objective': 'regression',\n",
    "        'n_jobs': -1,\n",
    "        'seed': 236,\n",
    "        'learning_rate': 0.3,\n",
    "        'bagging_fraction': 0.75,\n",
    "        'bagging_freq': 10, \n",
    "        'colsample_bytree': 0.75\n",
    "    }\n",
    "\n",
    "    train_set = lgb.Dataset(train_data[features], train_data[target].values)\n",
    "    val_set = lgb.Dataset(valid_data[features], valid_data[target].values)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params, \n",
    "        train_set, \n",
    "        num_boost_round = 2500, \n",
    "        early_stopping_rounds = 50, \n",
    "        valid_sets = [train_set, val_set], \n",
    "        verbose_eval = 100\n",
    "    )\n",
    "    val_pred = model.predict(valid_data[features], num_iteration=model.best_iteration)\n",
    "    val_score = mean_squared_error(val_pred, valid_data[target].values, squared=False)\n",
    "    valid_data['predict'] = val_pred\n",
    "    print(f'validation score is {val_score}')\n",
    "    eval_data['predict'] = model.predict(eval_data[features], num_iteration=model.best_iteration)\n",
    "    return valid_data, eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 2.22055\tvalid_1's rmse: 2.36614\n",
      "[200]\ttraining's rmse: 2.15739\tvalid_1's rmse: 2.36143\n",
      "Early stopping, best iteration is:\n",
      "[171]\ttraining's rmse: 2.1713\tvalid_1's rmse: 2.36023\n",
      "validation score is 2.3602285363386035\n"
     ]
    }
   ],
   "source": [
    "validt_result, result = run_lgb(train_data, features, target='sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop Null Rows.\n",
      "Train DataFrame Size: 19240518\n",
      "Valid DataFrame Size: 488396\n",
      "\n",
      "1 of 1 Fold:\n",
      "\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.10975\tvalid's rmse: 2.70882\n",
      "Early stopping, best iteration is:\n",
      "[95]\ttrain's rmse: 3.11524\tvalid's rmse: 2.70686\n"
     ]
    }
   ],
   "source": [
    "train_model(train_data, features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 収束が思ったとおりになっていないのでここからやり直し"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_wrmsse(eval_data, preds, eval_days=28):\n",
    "    def ordered_d_cols(df_cols):\n",
    "        return sorted(df_cols, key=lambda x: int((re.search(r\"\\d+\", x)).group(0)))\n",
    "    # Processing train data.\n",
    "    df = pd.read_pickle('../data/reduced/sales_train_validation.pkl')\n",
    "    train_idx_labels = df.iloc[:, :-eval_days].copy(deep=True)\n",
    "    # Processing eval label data.\n",
    "    eval_labels = pd.pivot_table(\n",
    "        eval_data, index='id', columns='d', values='sales', fill_value=0).reset_index()\n",
    "    eval_labels = eval_labels[ordered_d_cols(eval_labels.drop('id', axis=1).columns.tolist())]\n",
    "    # Processing eval predict data.\n",
    "    eval_data['pred'] = preds\n",
    "    pred_labels = pd.pivot_table(\n",
    "        eval_data, index='id', columns='d', values='pred', fill_value=0).reset_index()\n",
    "    pred_labels = pred_labels[ordered_d_cols(pred_labels.drop('id', axis=1).columns.tolist())]\n",
    "    # Estimate WRMSSE score.\n",
    "    calendar = pd.read_pickle('../data/reduced/calendar.pkl')\n",
    "    sell_prices = pd.read_pickle('../data/reduced/sell_prices.pkl')\n",
    "    e = WRMSSEEvaluator(train_idx_labels, eval_labels, calendar, sell_prices)\n",
    "    score = e.score(pred_labels)\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluation_model(eval_data, feature):\n",
    "    lgbm_model = load_pickle(f'result/model/{VERSION}.pkl')\n",
    "    preds = lgbm_model.predict(eval_data[feature].values)\n",
    "    metric_scores = {}\n",
    "    metric_scores['RMSE'] = mean_squared_error(eval_data['sales'].values, preds, squared=False)\n",
    "    metric_scores['WRMSSE'] = estimate_wrmsse(eval_data, preds)\n",
    "    for metric, score in metric_scores.items():\n",
    "        print(f'{metric}: {score}')\n",
    "    # Dump metric_scores as Json file.\n",
    "    with open(f'result/score/{VERSION}.json', 'w') as file:\n",
    "        json.dump(metric_scores, file, indent=4)\n",
    "    return metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-58-b6b6947fe7c6>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data['pred'] = preds\n",
      "100%|██████████| 42840/42840 [00:04<00:00, 9339.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.1240870807187533\n",
      "WRMSSE: 0.5906028752682618\n"
     ]
    }
   ],
   "source": [
    "metric_scores = evaluation_model(eval_data, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_file(submit_data, feature, score):\n",
    "    submission = pd.read_pickle('../data/reduced/sample_submission.pkl')['id'].to_frame()\n",
    "    lgbm_model = load_pickle(f'result/model/{VERSION}.pkl')\n",
    "\n",
    "    submit_data['pred'] = lgbm_model.predict(submit_data[feature].values)\n",
    "    sub_validation = pd.pivot(submit_data, index='id', columns='date', values='pred').reset_index()\n",
    "    sub_validation.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "    submission = pd.merge(submission, sub_validation, how='left', on='id')\n",
    "    submission.fillna(0, inplace=True)\n",
    "    submission.to_csv(f'submit/{VERSION}_{score:.04f}.csv.gz', index=False, compression='gzip')\n",
    "\n",
    "    print('Submit DataFrame:', submission.shape)\n",
    "    print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-f17d96207e01>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  submit_data['pred'] = lgbm_model.predict(submit_data[feature].values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit DataFrame: (60980, 29)\n",
      "                              id        F1        F2        F3        F4  \\\n",
      "0  HOBBIES_1_001_CA_1_validation  0.825113  0.722461  0.719059  0.753416   \n",
      "1  HOBBIES_1_002_CA_1_validation  0.390858  0.376044  0.358181  0.323824   \n",
      "2  HOBBIES_1_003_CA_1_validation  0.467663  0.423125  0.423125  0.423125   \n",
      "3  HOBBIES_1_004_CA_1_validation  1.825545  1.637808  1.637808  1.520746   \n",
      "4  HOBBIES_1_005_CA_1_validation  1.034260  0.852266  1.033663  1.044387   \n",
      "\n",
      "         F5        F6        F7        F8        F9       F10       F11  \\\n",
      "0  0.797248  0.994578  1.052715  0.847650  0.906042  0.794916  1.017704   \n",
      "1  0.402013  0.570770  0.594550  0.436204  0.402369  0.344073  0.262556   \n",
      "2  0.466957  0.675235  0.703648  0.559714  0.513385  0.426311  0.315070   \n",
      "3  1.794293  2.192388  2.293874  1.717844  1.731961  1.608565  1.485340   \n",
      "4  1.095348  1.367307  1.750122  1.219289  1.160974  0.985625  0.933823   \n",
      "\n",
      "        F12       F13       F14       F15       F16       F17       F18  \\\n",
      "0  1.136760  1.368302  1.245149  1.058993  0.940658  0.945616  0.944253   \n",
      "1  0.408839  0.587470  0.493430  0.385963  0.335227  0.341425  0.341425   \n",
      "2  0.508940  0.669708  0.541311  0.477046  0.417465  0.402785  0.443442   \n",
      "3  1.730515  2.570498  2.628481  1.900153  1.806098  1.878628  1.845412   \n",
      "4  1.100220  1.420340  1.240142  1.036660  0.957228  0.956095  0.989219   \n",
      "\n",
      "        F19       F20       F21       F22       F23       F24       F25  \\\n",
      "0  1.004326  1.454514  1.309211  0.989829  0.937745  0.937745  0.994948   \n",
      "1  0.409867  0.572299  0.568750  0.379765  0.335227  0.314350  0.345475   \n",
      "2  0.535300  0.687623  0.758871  0.530059  0.615510  0.504408  0.468052   \n",
      "3  2.018929  2.212055  2.308684  2.075332  1.810731  1.596608  1.581379   \n",
      "4  1.054760  1.335180  1.342546  1.036438  0.969374  0.931718  0.908327   \n",
      "\n",
      "        F26       F27       F28  \n",
      "0  1.018875  1.303384  1.293774  \n",
      "1  0.405801  0.565711  0.550981  \n",
      "2  0.562734  0.791125  0.791125  \n",
      "3  1.870407  2.803608  2.850119  \n",
      "4  1.026350  1.341141  1.497042  \n"
     ]
    }
   ],
   "source": [
    "create_submission_file(submit_data, features, metric_scores['WRMSSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
