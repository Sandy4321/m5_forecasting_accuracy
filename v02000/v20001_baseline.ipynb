{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v02000 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Baseline を見直す\n",
    "    - 予測変数を対数変換しない。\n",
    "    - モデルを poission から regression に変更する\n",
    "- [x] WRMSSE の評価関数クラスを定義し、評価を行う。\n",
    "- [x] Competition Evaluation と Training Evaluation を用意したい。\n",
    "    - 学習では柔軟度が高く、使いやすい関数を良いしたい。\n",
    "    - 評価では、できるだけLBと近い値で評価したい。\n",
    "- [x] WRMSSE の評価関数クラスを定義し、LightGBM の評価関数として使う\n",
    "- [ ] 損失関数にRMSLEを適応してみる。\n",
    "- [ ] 特徴量の追加\n",
    "    -  https://www.kaggle.com/rohitsingh9990/m5-lgbm-fe?scriptVersionId=30700291\n",
    "    - コスト関数を変えれば`0.55690`まで上がるらしい。\n",
    "    - 特徴量のcacheは、どの特徴量がどのような影響があるのかを実験するために、役割ごとで分けて作ったほうがよさそう。\n",
    "- [ ] Group K fold で学習してみる。\n",
    "    - https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv\n",
    "- [ ] Cut off lags nan rows\n",
    "- [ ] カテゴリごとの標準化でスコアが改善するか試す\n",
    "- [ ] 特徴量の追加、Aggregated Sales Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "VERSION = 'v02000'\n",
    "TARGET = 'sales'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# custom funcs\n",
    "from script import WRMSSEEvaluator\n",
    "from script import cache_result\n",
    "from script import reduce_mem_usage\n",
    "from script import load_pickle, dump_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data And Initial Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache to features/parse_calendar.pkl\n"
     ]
    }
   ],
   "source": [
    "@cache_result(filename='parse_calendar', use_cache=False)\n",
    "def parse_calendar():\n",
    "    calendar = pd.read_pickle('../data/reduced/calendar.pkl')\n",
    "    # fill null feature\n",
    "    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for f in nan_features:\n",
    "        calendar[f].fillna('null', inplace=True)\n",
    "    # label encoding\n",
    "    cat_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for f in cat_features:\n",
    "        encodaer = preprocessing.LabelEncoder()\n",
    "        calendar[f] = encodaer.fit_transform(calendar[f])\n",
    "        \n",
    "    calendar['date'] = pd.to_datetime(calendar['date'])\n",
    "    attrs = [\n",
    "        \"year\",\n",
    "#         \"quarter\",\n",
    "        \"month\",\n",
    "        \"week\",\n",
    "#         \"weekofyear\",\n",
    "        \"day\",\n",
    "        \"dayofweek\",\n",
    "#         \"is_year_end\",\n",
    "#         \"is_year_start\",\n",
    "#         \"is_quarter_end\",\n",
    "#         \"is_quarter_start\",\n",
    "#         \"is_month_end\",\n",
    "#         \"is_month_start\",\n",
    "    ]\n",
    "\n",
    "    for attr in attrs:\n",
    "        calendar[attr] = getattr(calendar['date'].dt, attr)\n",
    "#     calendar[\"is_weekend\"] = calendar[\"dayofweek\"].isin([5, 6]).astype(np.int8)|\n",
    "        \n",
    "#     drop_features = ['weekday', 'wday', 'month', 'year']\n",
    "#     features = calendar.columns.tolist()\n",
    "#     features = [f for f in features if f not in drop_features]\n",
    "    return calendar\n",
    " \n",
    "\n",
    "_ = parse_calendar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache_result(filename='parse_sell_prices', use_cache=True)\n",
    "def parse_sell_prices():\n",
    "    sell_prices = pd.read_pickle('../data/reduced/sell_prices.pkl')\n",
    "    return sell_prices\n",
    "\n",
    "_ = parse_sell_prices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache_result(filename='parse_sales_train', use_cache=True)\n",
    "def parse_sales_train():\n",
    "    train = pd.read_pickle('../data/reduced/sales_train_validation.pkl')\n",
    "    # Add Prediction Columns\n",
    "    start_d = 1914\n",
    "    end_d = 1969\n",
    "    for i in range(start_d, end_d+1):\n",
    "        train[f'd_{i}'] = 0\n",
    "    return train\n",
    "\n",
    "_ = parse_sales_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache_result(filename='melted_and_merged_train', use_cache=True)\n",
    "def melted_and_merged_train():\n",
    "    # Load Data\n",
    "    calendar = pd.read_pickle('features/parse_calendar.pkl')\n",
    "    sell_prices = pd.read_pickle('features/parse_sell_prices.pkl')\n",
    "    df = pd.read_pickle('features/parse_sales_train.pkl')\n",
    "    \n",
    "    idx_cols = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    df = pd.melt(df, id_vars=idx_cols, var_name='d', value_name='sales')\n",
    "    df = pd.merge(df, calendar, how='left', on='d')\n",
    "    df = pd.merge(df, sell_prices, how='left', on=['store_id', 'item_id', 'wm_yr_wk'])\n",
    "    \n",
    "    # Label Encoding\n",
    "    cat_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    for c in cat_cols:\n",
    "        encodaer = preprocessing.LabelEncoder()\n",
    "        df[c] = encodaer.fit_transform(df[c])\n",
    "        \n",
    "    df.dropna(subset=['sell_price'], axis=0, inplace=True)\n",
    "    return df.pipe(reduce_mem_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache_result(filename='simple_fe', use_cache=True)\n",
    "def simple_fe():\n",
    "    df = pd.read_pickle('features/melted_and_merged_train.pkl')\n",
    "    # rolling demand features\n",
    "    df['sales_lag_t28'] = df.groupby(['id'])['sales'].transform(lambda x: x.shift(28))\n",
    "    df['sales_lag_t29'] = df.groupby(['id'])['sales'].transform(lambda x: x.shift(29))\n",
    "    df['sales_lag_t30'] = df.groupby(['id'])['sales'].transform(lambda x: x.shift(30))\n",
    "    df['sales_rolling_mean_t7'] = df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(7).mean())\n",
    "    df['sales_rolling_std_t7'] = df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(7).std())\n",
    "    df['sales_rolling_mean_t30'] = df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(30).mean())\n",
    "    df['sales_rolling_mean_t90'] = df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(90).mean())\n",
    "    df['sales_rolling_mean_t180'] = df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(180).mean())\n",
    "    df['sales_rolling_std_t30'] = df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(30).std())\n",
    "    df['sales_rolling_skew_t30'] = df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(30).skew())\n",
    "    df['sales_rolling_kurt_t30'] = df.groupby(['id'])['sales'].transform(lambda x: x.shift(28).rolling(30).kurt())\n",
    "    \n",
    "    # price features\n",
    "    df['price_lag_t1'] = df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1)) # after drop.\n",
    "    df['price_change_t1'] = (df['price_lag_t1'] - df['sell_price']) / (df['price_lag_t1'])\n",
    "    df['rolling_price_max_t365'] = df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max()) # after drop.\n",
    "    df['price_change_t365'] = (df['rolling_price_max_t365'] - df['sell_price']) / (df['rolling_price_max_t365'])\n",
    "    df['price_rolling_std_t7'] = df.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n",
    "    df['price_rolling_std_t30'] = df.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std())\n",
    "    df.drop(['rolling_price_max_t365', 'price_lag_t1'], inplace = True, axis = 1)\n",
    "    \n",
    "    return df.pipe(reduce_mem_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = simple_fe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47735397, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>week</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>sales_lag_t28</th>\n",
       "      <th>sales_lag_t29</th>\n",
       "      <th>sales_lag_t30</th>\n",
       "      <th>sales_rolling_mean_t7</th>\n",
       "      <th>sales_rolling_std_t7</th>\n",
       "      <th>sales_rolling_mean_t30</th>\n",
       "      <th>sales_rolling_mean_t90</th>\n",
       "      <th>sales_rolling_mean_t180</th>\n",
       "      <th>sales_rolling_std_t30</th>\n",
       "      <th>sales_rolling_skew_t30</th>\n",
       "      <th>sales_rolling_kurt_t30</th>\n",
       "      <th>price_change_t1</th>\n",
       "      <th>price_change_t365</th>\n",
       "      <th>price_rolling_std_t7</th>\n",
       "      <th>price_rolling_std_t30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
       "      <td>1444</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>12</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0.459961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOBBIES_1_009_CA_1_validation</td>\n",
       "      <td>1445</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>1.559570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
       "      <td>1446</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>3.169922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
       "      <td>1448</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>5.980469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HOBBIES_1_015_CA_1_validation</td>\n",
       "      <td>1451</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0.700195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  item_id  dept_id  cat_id  store_id  \\\n",
       "7   HOBBIES_1_008_CA_1_validation     1444        3       1         0   \n",
       "8   HOBBIES_1_009_CA_1_validation     1445        3       1         0   \n",
       "9   HOBBIES_1_010_CA_1_validation     1446        3       1         0   \n",
       "11  HOBBIES_1_012_CA_1_validation     1448        3       1         0   \n",
       "14  HOBBIES_1_015_CA_1_validation     1451        3       1         0   \n",
       "\n",
       "    state_id    d  sales       date  wm_yr_wk   weekday  wday  month  year  \\\n",
       "7          0  d_1     12 2011-01-29     11101  Saturday     1      1  2011   \n",
       "8          0  d_1      2 2011-01-29     11101  Saturday     1      1  2011   \n",
       "9          0  d_1      0 2011-01-29     11101  Saturday     1      1  2011   \n",
       "11         0  d_1      0 2011-01-29     11101  Saturday     1      1  2011   \n",
       "14         0  d_1      4 2011-01-29     11101  Saturday     1      1  2011   \n",
       "\n",
       "    event_name_1  event_type_1  event_name_2  event_type_2  snap_CA  snap_TX  \\\n",
       "7             30             4             4             2        0        0   \n",
       "8             30             4             4             2        0        0   \n",
       "9             30             4             4             2        0        0   \n",
       "11            30             4             4             2        0        0   \n",
       "14            30             4             4             2        0        0   \n",
       "\n",
       "    snap_WI  week  day  dayofweek  sell_price  sales_lag_t28  sales_lag_t29  \\\n",
       "7         0     4   29          5    0.459961            NaN            NaN   \n",
       "8         0     4   29          5    1.559570            NaN            NaN   \n",
       "9         0     4   29          5    3.169922            NaN            NaN   \n",
       "11        0     4   29          5    5.980469            NaN            NaN   \n",
       "14        0     4   29          5    0.700195            NaN            NaN   \n",
       "\n",
       "    sales_lag_t30  sales_rolling_mean_t7  sales_rolling_std_t7  \\\n",
       "7             NaN                    NaN                   NaN   \n",
       "8             NaN                    NaN                   NaN   \n",
       "9             NaN                    NaN                   NaN   \n",
       "11            NaN                    NaN                   NaN   \n",
       "14            NaN                    NaN                   NaN   \n",
       "\n",
       "    sales_rolling_mean_t30  sales_rolling_mean_t90  sales_rolling_mean_t180  \\\n",
       "7                      NaN                     NaN                      NaN   \n",
       "8                      NaN                     NaN                      NaN   \n",
       "9                      NaN                     NaN                      NaN   \n",
       "11                     NaN                     NaN                      NaN   \n",
       "14                     NaN                     NaN                      NaN   \n",
       "\n",
       "    sales_rolling_std_t30  sales_rolling_skew_t30  sales_rolling_kurt_t30  \\\n",
       "7                     NaN                     NaN                     NaN   \n",
       "8                     NaN                     NaN                     NaN   \n",
       "9                     NaN                     NaN                     NaN   \n",
       "11                    NaN                     NaN                     NaN   \n",
       "14                    NaN                     NaN                     NaN   \n",
       "\n",
       "    price_change_t1  price_change_t365  price_rolling_std_t7  \\\n",
       "7               NaN                NaN                   NaN   \n",
       "8               NaN                NaN                   NaN   \n",
       "9               NaN                NaN                   NaN   \n",
       "11              NaN                NaN                   NaN   \n",
       "14              NaN                NaN                   NaN   \n",
       "\n",
       "    price_rolling_std_t30  \n",
       "7                     NaN  \n",
       "8                     NaN  \n",
       "9                     NaN  \n",
       "11                    NaN  \n",
       "14                    NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Evaluation Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42840/42840 [00:05<00:00, 8374.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache to features/evaluator.pkl\n"
     ]
    }
   ],
   "source": [
    "''' Note.\n",
    "- この関数の制約は\n",
    "    - validation では, すべての id が存在し, 連続する28日のデータであること.\n",
    "    - validation と prediction の id の順序が同一であること.\n",
    "'''\n",
    "class WRMSSEForLightGBM(WRMSSEEvaluator):\n",
    "    def feval(self, preds, dtrain):\n",
    "        row, col = self.valid_df[self.valid_target_columns].shape\n",
    "        preds = preds.reshape(col, row).T\n",
    "        \n",
    "        score = self.score(preds)\n",
    "        return 'WRMSSE', score, False\n",
    "    \n",
    "    def get_sample_weight(self, data_idx):\n",
    "        '''\n",
    "        sample weight for rmse.\n",
    "        Weights for doing WRMSSE-like evaluations using RMSE.\n",
    "        '''\n",
    "        data_idx = data_idx.apply(lambda x: x.rsplit('_', 1)[0]).values\n",
    "        \n",
    "        weight_df = self.weights * 12\n",
    "        weight_df.index = weight_df.index.str.replace('--', '_')\n",
    "        weight_df.columns = ['weight']\n",
    "        scale = np.where(self.scale != 0, self.scale, 1)\n",
    "        weight_df['sample_weight'] = weight_df['weight'] / scale\n",
    "\n",
    "        return weight_df.loc[data_idx, 'sample_weight'].values\n",
    "\n",
    "\n",
    "    \n",
    "@cache_result(filename='evaluator', use_cache=False)\n",
    "def get_evaluator():\n",
    "    train_df = pd.read_pickle('../data/reduced/sales_train_validation.pkl')\n",
    "\n",
    "    train_fold_df = train_df.iloc[:, :-28]\n",
    "    valid_fold_df = train_df.iloc[:, -28:].copy()\n",
    "\n",
    "    evaluator = WRMSSEForLightGBM(\n",
    "        train_df=train_fold_df, \n",
    "        valid_df=valid_fold_df, \n",
    "        calendar=pd.read_pickle('../data/reduced/calendar.pkl'), \n",
    "        prices=pd.read_pickle('../data/reduced/sell_prices.pkl')\n",
    "    )\n",
    "    \n",
    "    return evaluator\n",
    "    \n",
    "evaluator = get_evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(preds, actual):\n",
    "    return mean_squared_error(np.log1p(actual), np.log1p(preds), squared=False)\n",
    "\n",
    "def lgbm_rmsle(preds, data):\n",
    "    preds = np.clip(preds, 0, None)\n",
    "    return 'RMSLE', rmsle(preds, data.get_label()), False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Split Data '''\n",
    "def train_eval_submit_split(df, eval_days=28):\n",
    "    submit_mask = (df[\"date\"] >= '2016-04-25')\n",
    "\n",
    "    eval_date = datetime.datetime.strptime('2016-04-25', '%Y-%m-%d') - datetime.timedelta(days=eval_days)\n",
    "    eval_mask = ((df[\"date\"] >= eval_date) & (df[\"date\"] < '2016-04-25'))\n",
    "    \n",
    "    valid_date = datetime.datetime.strptime('2016-04-25', '%Y-%m-%d') - datetime.timedelta(days=eval_days*2)\n",
    "    valid_mask = ((df[\"date\"] >= eval_date) & (df[\"date\"] < '2016-04-25'))\n",
    "    \n",
    "    train_mask = ((~eval_mask) & (~submit_mask) & (~valid_mask))\n",
    "    return df[train_mask], df[valid_mask], df[eval_mask], df[submit_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, eval_data, sub_data = train_eval_submit_split(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'sales'\n",
    "features = [\n",
    "    'item_id',\n",
    "    'dept_id',\n",
    "    'cat_id',\n",
    "    'store_id',\n",
    "    'state_id',\n",
    "    'day',\n",
    "    'event_name_1',\n",
    "    'event_type_1',\n",
    "    'event_name_2',\n",
    "    'event_type_2',\n",
    "    'snap_CA',\n",
    "    'snap_TX',\n",
    "    'snap_WI',\n",
    "    'sell_price',\n",
    "    'sales_lag_t28',\n",
    "    'sales_lag_t29',\n",
    "    'sales_lag_t30',\n",
    "    'sales_rolling_mean_t7',\n",
    "    'sales_rolling_std_t7',\n",
    "    'sales_rolling_mean_t30',\n",
    "    'sales_rolling_mean_t90',\n",
    "    'sales_rolling_mean_t180',\n",
    "    'sales_rolling_std_t30',\n",
    "    'sales_rolling_skew_t30',\n",
    "    'sales_rolling_kurt_t30',\n",
    "    'price_change_t1',\n",
    "    'price_change_t365',\n",
    "    'price_rolling_std_t7',\n",
    "    'price_rolling_std_t30',\n",
    "    'year',\n",
    "    'month',\n",
    "    'week',\n",
    "    'dayofweek'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'rmse',\n",
    "    'objective': 'regression',\n",
    "    'n_jobs': -1,\n",
    "    'seed': SEED,\n",
    "    'learning_rate': 0.1,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 10, \n",
    "    'colsample_bytree': 0.75,\n",
    "#     'force_row_wise': True\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'num_boost_round': 2500, \n",
    "    'early_stopping_rounds': 50,\n",
    "    'verbose_eval': 100,\n",
    "    'feval': lgbm_rmsle\n",
    "#     'feval': evaluator.feval\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = lgb.Dataset(train_data[features], train_data[target])\n",
    "val_set = lgb.Dataset(valid_data[features], valid_data[target], reference=train_set)\n",
    "\n",
    "use_weight = False\n",
    "if use_weight:\n",
    "    train_set.set_weight(evaluator.get_sample_weight(train_data['id']))\n",
    "    val_set.set_weight(evaluator.get_sample_weight(valid_data['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(params, train_set, valid_sets=[train_set, val_set], **train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model, max_num_features=None, figsize=(10, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(eval_data[features])\n",
    "val_rmse = metrics.mean_squared_error(val_pred, eval_data[target], squared=False)\n",
    "print(f'Our val RMSE score is {val_rmse}')\n",
    "\n",
    "valid_preds = val_pred.reshape(28, -1).T\n",
    "valid_wrmsse = evaluator.score(valid_preds)\n",
    "print(f'Our val WRMSSE score is {valid_wrmsse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred = model.predict(sub_data[features])\n",
    "\n",
    "submission = sub_data[['id', 'd']].copy(deep=True)\n",
    "submission['sales'] = sub_pred\n",
    "submission = pd.pivot(submission, index='id', columns='d', values='sales').reset_index()\n",
    "# split valid and eval\n",
    "valid_sub = submission[['id']+[f'd_{i}' for i in range(1914, 1942)]]\n",
    "eval_sub = submission[['id']+[f'd_{i}' for i in range(1942, 1970)]]\n",
    "# rename columns\n",
    "valid_sub.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "eval_sub.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "# rename id evaluation\n",
    "eval_sub = eval_sub.assign(id=lambda x: x['id'].str.replace('validation', 'evaluation'))\n",
    "\n",
    "submission = pd.concat([valid_sub, eval_sub], axis=0)\n",
    "\n",
    "sample_submission = pd.read_pickle('../data/reduced/sample_submission.pkl')\n",
    "submission = sample_submission[['id']].merge(submission, how='left', on='id')\n",
    "submission.to_csv(f'submit/{VERSION}_{valid_wrmsse:.05}.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission.shape)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
