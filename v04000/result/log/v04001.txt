

--- Load Data and Initial Processing ---


Load Cached data, features/parse_calendar.pkl
Load Cached data, features/parse_sell_prices.pkl
Load Cached data, features/parse_sales_train.pkl


--- Transform ---


Load Cached data, features/melted_and_merged_train.pkl


--- Feature Engineering ---


Mem. usage decreased to 2476.65 Mb (75.0% reduction)
Cache to features/sales_lag_and_roll.pkl
Load Cached data, features/price_simple_feature.pkl
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 35093990 entries, 0 to 35093989
Data columns (total 80 columns):
 #   Column                        Dtype
---  ------                        -----
 0   id                            object
 1   item_id                       category
 2   dept_id                       category
 3   cat_id                        category
 4   store_id                      category
 5   state_id                      category
 6   d                             object
 7   sales                         int16
 8   date                          datetime64[ns]
 9   wm_yr_wk                      int16
 10  event_name_1                  category
 11  event_type_1                  category
 12  event_name_2                  category
 13  event_type_2                  category
 14  snap_CA                       int8
 15  snap_TX                       int8
 16  snap_WI                       int8
 17  year                          int16
 18  quarter                       int8
 19  month                         int8
 20  week                          int8
 21  weekofyear                    int8
 22  day                           int8
 23  dayofweek                     int8
 24  dayofyear                     int16
 25  is_year_end                   bool
 26  is_year_start                 bool
 27  is_quarter_end                bool
 28  is_quarter_start              bool
 29  is_month_end                  bool
 30  is_month_start                bool
 31  is_weekend                    bool
 32  sell_price                    float16
 33  release                       float16
 34  price_max                     float16
 35  price_min                     float16
 36  price_std                     float16
 37  price_mean                    float16
 38  price_nunique                 float16
 39  id_nunique_by_price           float16
 40  price_norm                    float16
 41  sales_lag_t28p0               float16
 42  sales_lag_t28p1               float16
 43  sales_lag_t28p2               float16
 44  sales_lag_t28p3               float16
 45  sales_lag_t28p4               float16
 46  sales_lag_t28p5               float16
 47  sales_lag_t28p6               float16
 48  sales_lag_t28p7               float16
 49  sales_lag_t28p8               float16
 50  sales_lag_t28p9               float16
 51  sales_lag_t28p10              float16
 52  sales_lag_t28p11              float16
 53  sales_lag_t28p12              float16
 54  sales_lag_t28p13              float16
 55  sales_lag_t28p14              float16
 56  sales_roll_mean_t7            float16
 57  sales_roll_std_t7             float16
 58  sales_rolling_ZeroRatio_t7    float16
 59  sales_rolling_ZeroCount_t7    float16
 60  sales_roll_mean_t14           float16
 61  sales_roll_std_t14            float16
 62  sales_rolling_ZeroRatio_t14   float16
 63  sales_rolling_ZeroCount_t14   float16
 64  sales_roll_mean_t30           float16
 65  sales_roll_std_t30            float16
 66  sales_rolling_ZeroRatio_t30   float16
 67  sales_rolling_ZeroCount_t30   float16
 68  sales_roll_mean_t60           float16
 69  sales_roll_std_t60            float16
 70  sales_rolling_ZeroRatio_t60   float16
 71  sales_rolling_ZeroCount_t60   float16
 72  sales_roll_mean_t180          float16
 73  sales_roll_std_t180           float16
 74  sales_rolling_ZeroRatio_t180  float16
 75  sales_rolling_ZeroCount_t180  float16
 76  sales_rolling_skew_t30        float16
 77  sales_rolling_kurt_t30        float16
 78  price_momentum                float16
 79  price_momentum_m              float16
dtypes: bool(7), category(9), datetime64[ns](1), float16(48), int16(4), int8(9), object(2)
memory usage: 5.0+ GB

 None
Cache Train and Submission Data.


--- Train Model ---


Define Evaluation Object.
Parameters:
 {
    "model_params": {
        "boosting": "gbdt",
        "objective": "tweedie",
        "tweedie_variance_power": 1.1,
        "metric": "None",
        "num_leaves": 127,
        "min_data_in_leaf": 25,
        "seed": 42,
        "learning_rate": 0.1,
        "subsample": 0.5,
        "subsample_freq": 1,
        "feature_fraction": 0.8,
        "force_row_wise": true,
        "verbose": -1
    },
    "train_params": {
        "num_boost_round": 1500,
        "early_stopping_rounds": 100,
        "verbose_eval": 100
    }
}

Training until validation scores don't improve for 100 rounds
[100]	valid_0's WRMSSE: 0.530142
[200]	valid_0's WRMSSE: 0.523584
[300]	valid_0's WRMSSE: 0.521729
[400]	valid_0's WRMSSE: 0.519869
[500]	valid_0's WRMSSE: 0.521288
Early stopping, best iteration is:
[437]	valid_0's WRMSSE: 0.518965

Evaluation:
Our val RMSE score is 2.1266648913301758
Our val WRMSSE score is 0.5189654429316758


--- Submission ---


(60980, 29)
                              id        F1        F2        F3        F4  \
0  HOBBIES_1_001_CA_1_validation  0.810970  0.735649  0.730669  0.710223
1  HOBBIES_1_002_CA_1_validation  0.256381  0.259098  0.252131  0.250208
2  HOBBIES_1_003_CA_1_validation  0.323904  0.325362  0.319062  0.322389
3  HOBBIES_1_004_CA_1_validation  1.994676  1.689544  1.590930  1.610534
4  HOBBIES_1_005_CA_1_validation  1.044572  0.894120  0.927723  0.989320

         F5        F6        F7        F8        F9       F10       F11  \
0  0.804160  0.938028  0.975425  0.682379  0.706131  0.747227  0.723449
1  0.319935  0.389570  0.429171  0.243801  0.239392  0.226793  0.228675
2  0.459886  0.488038  0.738736  0.382085  0.395615  0.365512  0.399247
3  2.157757  2.566429  2.818951  1.940211  1.766304  1.498424  1.474825
4  1.144289  1.401992  1.656324  1.189478  1.203317  1.016254  1.026897

        F12       F13       F14       F15       F16       F17       F18  \
0  0.863400  1.149028  0.855091  0.845654  0.740082  0.706994  0.767219
1  0.271043  0.312230  0.254342  0.176406  0.180776  0.173218  0.178809
2  0.455635  0.631678  0.594884  0.306326  0.324616  0.342669  0.370824
3  2.019384  2.720969  2.118655  1.812441  1.560177  1.568313  1.502444
4  1.081754  1.379237  1.015105  1.146518  0.973266  1.007488  1.019091

        F19       F20       F21       F22       F23       F24       F25  \
0  0.858986  1.099770  1.019209  0.819860  0.701160  0.684252  0.667923
1  0.218836  0.283765  0.274905  0.185190  0.184589  0.211842  0.219269
2  0.492876  0.607556  0.673464  0.342438  0.357237  0.365573  0.357966
3  1.746905  2.390139  2.790751  1.875178  1.546765  1.454950  1.428854
4  1.115921  1.460192  1.460182  1.045559  0.958438  0.933996  0.978348

        F26       F27       F28
0  0.786206  1.079626  1.017360
1  0.274285  0.314029  0.302906
2  0.474170  0.603943  0.598999
3  1.861286  2.672595  2.949280
4  1.125225  1.458488  1.534260
